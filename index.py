import requests
from bs4 import BeautifulSoup
import time
import os
import json
import re
from urllib.parse import urljoin, urlparse

# Wordæ–‡æ¡£ç›¸å…³å¯¼å…¥
try:
    from docx import Document
    from docx.shared import Inches, Pt, RGBColor
    from docx.enum.text import WD_ALIGN_PARAGRAPH
    from docx.oxml.ns import qn
    DOCX_AVAILABLE = True
    print("âœ… python-docx å·²å®‰è£…ï¼Œå°†è‡ªåŠ¨ç”ŸæˆWordæ–‡æ¡£")
except ImportError:
    DOCX_AVAILABLE = False
    print("âš ï¸  python-docx æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install python-docx")
    print("   å°†åªç”ŸæˆTXTå’ŒJSONæ–‡ä»¶")

class UniversalNovelCrawler:
    def __init__(self, catalog_url=None):
        self.catalog_url = catalog_url
        self.base_domain = self.get_base_domain(catalog_url) if catalog_url else ""
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Connection': 'keep-alive',
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        self.book_info = {'title': 'æœªçŸ¥ä¹¦å', 'author': 'æœªçŸ¥ä½œè€…'}
        
    def get_base_domain(self, url):
        """è·å–URLçš„åŸºç¡€åŸŸå"""
        if not url:
            return ""
        parsed = urlparse(url)
        return f"{parsed.scheme}://{parsed.netloc}"
    
    def get_book_info_from_catalog(self, soup):
        """ä»ç›®å½•é¡µé¢æå–ä¹¦åå’Œä½œè€…ä¿¡æ¯"""
        try:
            # æå–ä¹¦å
            title_selectors = ['h1', 'h2', '.book-title', '.title', '#title', '.book-name', '[class*="title"]']
            book_title = "æœªçŸ¥ä¹¦å"
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    title_text = title_elem.get_text(strip=True)
                    if title_text and 3 < len(title_text) < 50:
                        book_title = title_text
                        break
            
            # æå–ä½œè€…
            author_selectors = ['.author', '.book-author', '#author', '[class*="author"]', '.writer']
            book_author = "æœªçŸ¥ä½œè€…"
            for selector in author_selectors:
                author_elem = soup.select_one(selector)
                if author_elem:
                    author_text = re.sub(r'ä½œè€…[ï¼š:]\s*', '', author_elem.get_text(strip=True))
                    if author_text and len(author_text) < 30:
                        book_author = author_text
                        break
            
            # ä»æ ‡é¢˜ä¸­æå–ä½œè€…
            if "ä½œè€…" in book_title:
                parts = re.split(r'[ä½œè€…ï¼š:]', book_title)
                if len(parts) >= 2:
                    book_title = parts[0].strip()
                    book_author = parts[1].strip()
            
            self.book_info = {'title': book_title, 'author': book_author}
            print(f"ğŸ“š ä¹¦å: {book_title} | ä½œè€…: {book_author}")
            
        except Exception as e:
            print(f"âš ï¸  è·å–ä¹¦ç±ä¿¡æ¯å¤±è´¥: {e}")
    
    def parse_chapter_list(self):
        """è§£æç« èŠ‚åˆ—è¡¨"""
        print(f"ğŸ” æ­£åœ¨è§£æç›®å½•é¡µé¢: {self.catalog_url}")
        
        try:
            response = self.session.get(self.catalog_url, timeout=15)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                print(f"âŒ ç›®å½•é¡µé¢è®¿é—®å¤±è´¥: {response.status_code}")
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            self.get_book_info_from_catalog(soup)
            
            chapters = []
            
            # ç­–ç•¥1: å¯»æ‰¾ç« èŠ‚å®¹å™¨
            container_selectors = [
                '.chapter-list', '.catalogue', '.catalog', '.list', '.content-list', 
                '#list', '#catalog', '[class*="chapter"]', '[class*="catalog"]', 'ul', 'ol'
            ]
            
            for selector in container_selectors:
                container = soup.select_one(selector)
                if container:
                    links = container.find_all('a', href=True)
                    if len(links) >= 3:
                        print(f"âœ… æ‰¾åˆ°ç« èŠ‚å®¹å™¨: {selector} ({len(links)} ä¸ªé“¾æ¥)")
                        chapters = self._extract_chapters_from_links(links)
                        break
            
            # ç­–ç•¥2: æ¨¡å¼åŒ¹é…æŸ¥æ‰¾ç« èŠ‚
            if not chapters:
                print("ğŸ”§ åœ¨æ•´ä¸ªé¡µé¢æŸ¥æ‰¾ç« èŠ‚é“¾æ¥...")
                all_links = soup.find_all('a', href=True)
                chapters = self._extract_chapters_from_links(all_links, pattern_match=True)
            
            # å»é‡å¹¶æ’åº
            unique_chapters = self._deduplicate_chapters(chapters)
            if unique_chapters:
                sorted_chapters = self.sort_chapters(unique_chapters)
                print(f"ğŸ“‹ å…±è§£æåˆ° {len(sorted_chapters)} ä¸ªç« èŠ‚")
                self._preview_chapters(sorted_chapters)
                return sorted_chapters
            
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•ç« èŠ‚")
            return []
            
        except Exception as e:
            print(f"âŒ è§£æå¤±è´¥: {e}")
            return []
    
    def _extract_chapters_from_links(self, links, pattern_match=False):
        """ä»é“¾æ¥ä¸­æå–ç« èŠ‚ä¿¡æ¯"""
        chapters = []
        skip_keywords = ['é¦–é¡µ', 'ä¹¦æ¶', 'ç™»å½•', 'æ³¨å†Œ', 'æœç´¢', 'æ’è¡Œ', 'home', 'login', 
                        'register', 'search', 'rank', 'ä¸Šä¸€é¡µ', 'ä¸‹ä¸€é¡µ', 'è¿”å›', 'ç›®å½•']
        
        for link in links:
            href = link.get('href')
            title = link.get_text(strip=True)
            
            if not title or len(title) > 100:
                continue
                
            # è¿‡æ»¤å¯¼èˆªé“¾æ¥
            if any(skip in title.lower() for skip in skip_keywords):
                continue
            
            # å¦‚æœéœ€è¦æ¨¡å¼åŒ¹é…ï¼Œæ£€æŸ¥æ˜¯å¦åƒç« èŠ‚æ ‡é¢˜
            if pattern_match:
                if not self._is_chapter_title(title):
                    continue
            
            full_url = urljoin(self.catalog_url, href)
            chapters.append({'title': title, 'url': full_url})
        
        return chapters
    
    def _is_chapter_title(self, title):
        """åˆ¤æ–­æ˜¯å¦æ˜¯ç« èŠ‚æ ‡é¢˜"""
        patterns = [
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+[ç« å›èŠ‚å·é›†éƒ¨]',
            r'[ç¬¬]?\d+[ç« å›èŠ‚å·é›†éƒ¨]',
            r'chapter\s*\d+',
        ]
        return any(re.search(pattern, title, re.I) for pattern in patterns) or 'ç« ' in title or 'å›' in title
    
    def _deduplicate_chapters(self, chapters):
        """å»é‡"""
        unique_chapters = []
        seen_urls = set()
        for chapter in chapters:
            if chapter['url'] not in seen_urls:
                unique_chapters.append(chapter)
                seen_urls.add(chapter['url'])
        return unique_chapters
    
    def _preview_chapters(self, chapters):
        """é¢„è§ˆç« èŠ‚"""
        print("ğŸ“– ç« èŠ‚é¢„è§ˆ:")
        for i, chapter in enumerate(chapters[:5], 1):
            print(f"   {i}. {chapter['title']}")
        if len(chapters) > 5:
            print(f"   ... è¿˜æœ‰ {len(chapters) - 5} ä¸ªç« èŠ‚")
    
    def sort_chapters(self, chapters):
        """å¯¹ç« èŠ‚è¿›è¡Œæ’åº - ä¿®æ­£æ’åºé—®é¢˜"""
        print("ğŸ”„ æ­£åœ¨æ’åºç« èŠ‚...")
        
        def get_sort_key(title):
            """æå–æ’åºå…³é”®å­— - ä¿®æ­£ç‰ˆæœ¬"""
            title_lower = title.lower().strip()
            
            # ç‰¹æ®Šç« èŠ‚å¤„ç†
            special_order = {
                'åºç« ': (0, 0), 'åºè¨€': (0, 0), 'åº': (0, 0), 'å‰è¨€': (0, 0),
                'æ¥”å­': (1, 0), 'å¼•å­': (1, 0), 'å¼€ç¯‡': (1, 0),
                'ç»ˆç« ': (999, 0), 'å°¾å£°': (999, 0), 'åè®°': (999, 0), 'ç•ªå¤–': (1000, 0)
            }
            
            for keyword, order in special_order.items():
                if keyword in title_lower:
                    return order
            
            # æå–ç« èŠ‚å· - ä¼˜åŒ–ç‰ˆæœ¬
            patterns = [
                (r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡]+)[ç« å›èŠ‚å·é›†éƒ¨ç¯‡]', self._chinese_to_number),
                (r'ç¬¬(\d+)[ç« å›èŠ‚å·é›†éƒ¨ç¯‡]', int),
                (r'^(\d+)[ç« å›èŠ‚å·é›†éƒ¨ç¯‡]', int),
                (r'chapter\s*(\d+)', int),
                (r'[ç« å›èŠ‚å·é›†éƒ¨ç¯‡](\d+)', int),
                (r'^(\d+)', int),
            ]
            
            for pattern, converter in patterns:
                match = re.search(pattern, title_lower)
                if match:
                    try:
                        num = converter(match.group(1))
                        return (10, num)  # æ­£å¸¸ç« èŠ‚
                    except:
                        continue
            
            # æ— æ³•è¯†åˆ«çš„æ”¾æœ€å
            return (500, hash(title) % 1000)  # ä½¿ç”¨hashä¿è¯ç¨³å®šæ’åº
        
        # æ’åºå¹¶æ·»åŠ è°ƒè¯•ä¿¡æ¯
        chapters_with_key = [(get_sort_key(ch['title']), ch) for ch in chapters]
        chapters_with_key.sort(key=lambda x: x[0])
        
        sorted_chapters = [ch for _, ch in chapters_with_key]
        
        print(f"âœ… æ’åºå®Œæˆï¼Œå‰10ç« :")
        for i, chapter in enumerate(sorted_chapters[:10], 1):
            print(f"   {i}. {chapter['title']}")
        
        return sorted_chapters
    
    def _chinese_to_number(self, chinese_str):
        """ä¸­æ–‡æ•°å­—è½¬æ¢"""
        chinese_dict = {
            'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4, 'äº”': 5, 'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9, 'å': 10,
            'åä¸€': 11, 'åäºŒ': 12, 'åä¸‰': 13, 'åå››': 14, 'åäº”': 15, 'åå…­': 16, 'åä¸ƒ': 17, 'åå…«': 18, 'åä¹': 19, 'äºŒå': 20,
            'äºŒåä¸€': 21, 'äºŒåäºŒ': 22, 'äºŒåä¸‰': 23, 'äºŒåå››': 24, 'äºŒåäº”': 25, 'äºŒåå…­': 26, 'äºŒåä¸ƒ': 27, 'äºŒåå…«': 28, 'äºŒåä¹': 29, 'ä¸‰å': 30,
            'ä¸‰åä¸€': 31, 'ä¸‰åäºŒ': 32, 'ä¸‰åä¸‰': 33, 'ä¸‰åå››': 34, 'ä¸‰åäº”': 35, 'ä¸‰åå…­': 36, 'ä¸‰åä¸ƒ': 37, 'ä¸‰åå…«': 38, 'ä¸‰åä¹': 39, 'å››å': 40,
            'å››åä¸€': 41, 'å››åäºŒ': 42, 'å››åä¸‰': 43, 'å››åå››': 44, 'å››åäº”': 45, 'å››åå…­': 46, 'å››åä¸ƒ': 47, 'å››åå…«': 48, 'å››åä¹': 49, 'äº”å': 50,
            'äº”åä¸€': 51, 'äº”åäºŒ': 52, 'äº”åä¸‰': 53, 'äº”åå››': 54, 'äº”åäº”': 55, 'äº”åå…­': 56, 'äº”åä¸ƒ': 57, 'äº”åå…«': 58, 'äº”åä¹': 59, 'å…­å': 60,
            'å…­åä¸€': 61, 'å…­åäºŒ': 62, 'å…­åä¸‰': 63, 'å…­åå››': 64, 'å…­åäº”': 65, 'å…­åå…­': 66, 'å…­åä¸ƒ': 67, 'å…­åå…«': 68, 'å…­åä¹': 69, 'ä¸ƒå': 70,
            'ä¸ƒåä¸€': 71, 'ä¸ƒåäºŒ': 72, 'ä¸ƒåä¸‰': 73, 'ä¸ƒåå››': 74, 'ä¸ƒåäº”': 75, 'ä¸ƒåå…­': 76, 'ä¸ƒåä¸ƒ': 77, 'ä¸ƒåå…«': 78, 'ä¸ƒåä¹': 79, 'å…«å': 80,
            'å…«åä¸€': 81, 'å…«åäºŒ': 82, 'å…«åä¸‰': 83, 'å…«åå››': 84, 'å…«åäº”': 85, 'å…«åå…­': 86, 'å…«åä¸ƒ': 87, 'å…«åå…«': 88, 'å…«åä¹': 89, 'ä¹å': 90,
            'ä¹åä¸€': 91, 'ä¹åäºŒ': 92, 'ä¹åä¸‰': 93, 'ä¹åå››': 94, 'ä¹åäº”': 95, 'ä¹åå…­': 96, 'ä¹åä¸ƒ': 97, 'ä¹åå…«': 98, 'ä¹åä¹': 99, 'ä¸€ç™¾': 100,
        }
        
        if chinese_str in chinese_dict:
            return chinese_dict[chinese_str]
        
        # å¤„ç†ç™¾ä½æ•°
        if 'ç™¾' in chinese_str:
            parts = chinese_str.split('ç™¾')
            if len(parts) == 2:
                hundred = chinese_dict.get(parts[0], 1) * 100
                remainder = chinese_dict.get(parts[1], 0) if parts[1] else 0
                return hundred + remainder
        
        return 0
    
    def get_chapter_content(self, chapter_url, chapter_title):
        """è·å–ç« èŠ‚å†…å®¹ - ä¿æŒåŸæœ‰çš„æ ‡é¢˜å¤„ç†é€»è¾‘"""
        try:
            print(f"  ğŸ“– è·å–: {chapter_title}")
            
            response = self.session.get(chapter_url, timeout=15)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                print(f"    âŒ HTTPé”™è¯¯: {response.status_code}")
                return {"title": chapter_title, "content": ""}
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                element.decompose()
            
            content = self._extract_content(soup)
            
            if content:
                # æ™ºèƒ½æ ‡é¢˜åˆå¹¶å¤„ç† - ä¿æŒåŸæœ‰é€»è¾‘
                merged_title, cleaned_content = self.merge_titles(chapter_title, content)
                paragraph_count = cleaned_content.count('\n\n') + 1
                
                if len(cleaned_content) > 50:
                    print(f"    âœ… æˆåŠŸ ({paragraph_count} ä¸ªæ®µè½)")
                    if merged_title != chapter_title:
                        print(f"    ğŸ”— æ ‡é¢˜å·²åˆå¹¶")
                    return {"title": merged_title, "content": cleaned_content}
            
            print(f"    âŒ å†…å®¹æå–å¤±è´¥")
            return {"title": chapter_title, "content": ""}
            
        except Exception as e:
            print(f"    âŒ è·å–å¤±è´¥: {e}")
            return {"title": chapter_title, "content": ""}
    
    def _extract_content(self, soup):
        """æå–æ­£æ–‡å†…å®¹"""
        # ç­–ç•¥1: å¯»æ‰¾å†…å®¹å®¹å™¨
        content_selectors = [
            '.content', '.main-content', '.chapter-content', '.text-content',
            '#content', '#main-content', '#chapter-content',
            '.post-content', '.entry-content', '.article-content',
            '.main .content', '.container .content', '[class*="content"]',
            '.main3 .left .cont', '.cont', '.chapter', '.article'
        ]
        
        for selector in content_selectors:
            container = soup.select_one(selector)
            if container:
                # æå–pæ ‡ç­¾æ®µè½
                paragraphs = container.find_all('p')
                if paragraphs:
                    para_texts = [p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 5]
                    if para_texts:
                        return '\n\n'.join(para_texts)
                
                # å°è¯•å…¶ä»–æ®µè½ç»“æ„
                divs = container.find_all(['div', 'span'])
                if len(divs) > 1:
                    div_texts = [div.get_text(strip=True) for div in divs if len(div.get_text(strip=True)) > 10]
                    if len(div_texts) > 1:
                        return '\n\n'.join(div_texts)
        
        # ç­–ç•¥2: å…¨é¡µé¢pæ ‡ç­¾
        all_paragraphs = soup.find_all('p')
        if all_paragraphs:
            skip_keywords = ['å¯¼èˆª', 'èœå•', 'ç™»å½•', 'ç‰ˆæƒ', 'copyright', 'ä¸Šä¸€ç« ', 'ä¸‹ä¸€ç« ']
            para_texts = []
            for p in all_paragraphs:
                text = p.get_text(strip=True)
                if (text and len(text) > 15 and 
                    not any(skip in text.lower() for skip in skip_keywords)):
                    para_texts.append(text)
            
            if len(para_texts) > 2:
                return '\n\n'.join(para_texts)
        
        return ""
    
    # ä¿æŒåŸæœ‰çš„æ ‡é¢˜å¤„ç†é€»è¾‘ - ç”¨æˆ·å¼ºè°ƒè¿™éƒ¨åˆ†å¾ˆå…³é”®
    def merge_titles(self, catalog_title, content):
        """æ™ºèƒ½åˆå¹¶ç›®å½•æ ‡é¢˜å’Œå†…å®¹æ ‡é¢˜"""
        if not content:
            return catalog_title, content
        
        lines_double = content.split('\n\n')
        lines_single = content.split('\n')
        
        first_line = ""
        remaining_content = content
        
        if lines_double and lines_double[0].strip():
            first_line = lines_double[0].strip()
            if len(first_line) <= 150:
                remaining_content = '\n\n'.join(lines_double[1:]).strip() if len(lines_double) > 1 else ""
            else:
                if lines_single and lines_single[0].strip():
                    first_line = lines_single[0].strip()
                    remaining_content = '\n'.join(lines_single[1:]).strip() if len(lines_single) > 1 else ""
        elif lines_single and lines_single[0].strip():
            first_line = lines_single[0].strip()
            remaining_content = '\n'.join(lines_single[1:]).strip() if len(lines_single) > 1 else ""
        
        if not first_line:
            return catalog_title, content
        
        print(f"    ğŸ” æ£€æŸ¥ç¬¬ä¸€è¡Œ: {first_line[:50]}{'...' if len(first_line) > 50 else ''}")
        
        if self.is_likely_title(first_line):
            merged_title = self.combine_titles(catalog_title, first_line)
            print(f"    ğŸ”— æ£€æµ‹åˆ°å†…å®¹æ ‡é¢˜ï¼Œå·²åˆå¹¶")
            return merged_title, remaining_content
        else:
            print(f"    âŒ ç¬¬ä¸€è¡Œä¸è¢«è¯†åˆ«ä¸ºæ ‡é¢˜")
            return catalog_title, content
    
    def is_likely_title(self, text):
        """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦å¯èƒ½æ˜¯æ ‡é¢˜"""
        if not text or len(text) > 200 or len(text) < 3:
            return False
        
        print(f"    ğŸ” æ ‡é¢˜åˆ¤æ–­: '{text}'")
        
        # å¼ºæ ‡é¢˜ç‰¹å¾
        strong_patterns = [
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+[ç« å›èŠ‚å·é›†éƒ¨ç¯‡]',
            r'^[ç¬¬]?\d+[ç« å›èŠ‚å·é›†éƒ¨ç¯‡]',
            r'chapter\s*\d+',
            r'^[\d\s\-\.]+[ç« å›èŠ‚å·é›†éƒ¨ç¯‡]',
            r'^ç¬¬.*[ç« å›èŠ‚å·é›†éƒ¨ç¯‡]',
        ]
        
        for pattern in strong_patterns:
            if re.search(pattern, text, re.I):
                print(f"    âœ… åŒ¹é…å¼ºæ ‡é¢˜æ¨¡å¼")
                return True
        
        # ç»¼åˆè¯„åˆ†
        score = 0
        chapter_keywords = ['ç« ', 'å›', 'èŠ‚', 'å·', 'é›†', 'éƒ¨', 'ç¯‡', 'chapter']
        if any(keyword in text.lower() for keyword in chapter_keywords):
            score += 2
        if re.search(r'\d+', text):
            score += 1
        if not text.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?')):
            score += 1
        if 5 <= len(text) <= 50:
            score += 1
        
        punctuation_ratio = len(re.findall(r'[ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹]', text)) / len(text)
        if punctuation_ratio < 0.3:
            score += 1
        
        print(f"    ğŸ“Š ç»¼åˆå¾—åˆ†: {score}/6")
        return score >= 3
    
    def combine_titles(self, catalog_title, content_title):
        """åˆå¹¶ç›®å½•æ ‡é¢˜å’Œå†…å®¹æ ‡é¢˜"""
        catalog_clean = catalog_title.strip()
        content_clean = content_title.strip()
        
        print(f"    ğŸ”— åˆå¹¶: ç›®å½•='{catalog_clean}' | å†…å®¹='{content_clean}'")
        
        if content_clean in catalog_clean:
            return catalog_clean
        if catalog_clean in content_clean:
            return content_clean
        
        # æ£€æŸ¥ç« èŠ‚å·
        catalog_num = self.extract_chapter_number(catalog_clean)
        content_num = self.extract_chapter_number(content_clean)
        
        if catalog_num and content_num and catalog_num == content_num:
            catalog_without_num = re.sub(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+[ç« å›èŠ‚å·é›†éƒ¨ç¯‡]\s*', '', catalog_clean)
            content_without_num = re.sub(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+[ç« å›èŠ‚å·é›†éƒ¨ç¯‡]\s*', '', content_clean)
            content_without_num = re.sub(r'^\d+[ç« å›èŠ‚å·é›†éƒ¨ç¯‡]\s*', '', content_without_num)
            
            if catalog_without_num and content_without_num and catalog_without_num != content_without_num:
                return f"{catalog_clean} {content_without_num}"
            else:
                return catalog_clean
        
        return f"{catalog_clean} {content_clean}"
    
    def extract_chapter_number(self, title):
        """æå–ç« èŠ‚å·"""
        patterns = [
            r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡]+)[ç« å›èŠ‚å·é›†éƒ¨ç¯‡]',
            r'ç¬¬(\d+)[ç« å›èŠ‚å·é›†éƒ¨ç¯‡]',
            r'^(\d+)[ç« å›èŠ‚å·é›†éƒ¨ç¯‡]',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, title)
            if match:
                return match.group(1)
        return None
    
    def crawl_book(self, delay=3, test_mode=False):
        """çˆ¬å–æ•´æœ¬ä¹¦"""
        mode_text = "æµ‹è¯•æ¨¡å¼ï¼ˆå‰3ç« ï¼‰" if test_mode else "å®Œæ•´æ¨¡å¼"
        print(f"ğŸš€ å¼€å§‹çˆ¬å–ã€Š{self.book_info['title']}ã€‹ - {mode_text}")
        print("=" * 60)
        
        chapters = self.parse_chapter_list()
        if not chapters:
            print("âŒ æ— æ³•è·å–ç« èŠ‚ä¿¡æ¯")
            return
        
        if test_mode:
            chapters = chapters[:3]
            print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šçˆ¬å–å‰ {len(chapters)} ç« ")
        
        print(f"ğŸ“š å¼€å§‹çˆ¬å– {len(chapters)} ä¸ªç« èŠ‚...")
        print("-" * 60)
        
        chapters_data = []
        success_count = 0
        
        for i, chapter in enumerate(chapters, 1):
            print(f"[{i:3d}/{len(chapters)}] {chapter['title']}")
            
            result = self.get_chapter_content(chapter['url'], chapter['title'])
            content = result["content"]
            paragraph_count = content.count('\n\n') + 1 if content else 0
            
            chapter_data = {
                'original_title': chapter['title'],
                'title': result["title"],
                'url': chapter['url'],
                'content': content,
                'char_count': len(content),
                'paragraph_count': paragraph_count,
                'success': len(content) > 50 and paragraph_count >= 1
            }
            
            chapters_data.append(chapter_data)
            
            if chapter_data['success']:
                success_count += 1
            
            if i < len(chapters):
                print(f"           â³ ç­‰å¾… {delay} ç§’...")
                time.sleep(delay)
        
        # ä¿å­˜ç»“æœ
        print("-" * 60)
        total_chars = sum(ch['char_count'] for ch in chapters_data)
        total_paragraphs = sum(ch['paragraph_count'] for ch in chapters_data)
        print(f"ğŸ“‹ å®Œæˆ: {success_count}/{len(chapters_data)} ç« èŠ‚, {total_chars:,} å­—, {total_paragraphs} æ®µè½")
        
        if success_count > 0:
            folder_name = self._save_results(chapters_data)
            print(f"ğŸ‰ æ–‡ä»¶å·²ä¿å­˜åˆ°: {folder_name}")
            return folder_name
        else:
            print("âŒ æ²¡æœ‰æˆåŠŸè·å–ä»»ä½•å†…å®¹")
    
    def _save_results(self, chapters_data):
        """ä¿å­˜æ‰€æœ‰æ ¼å¼çš„æ–‡ä»¶"""
        # åˆ›å»ºæ–‡ä»¶å¤¹
        safe_title = re.sub(r'[<>:"/\\|?*]', '_', self.book_info['title']).strip('_').strip()
        if not safe_title:
            safe_title = f"å°è¯´_{int(time.time())}"
        
        os.makedirs(safe_title, exist_ok=True)
        
        # ä¿å­˜æ–‡ä»¶
        txt_path = os.path.join(safe_title, f"{safe_title}.txt")
        json_path = os.path.join(safe_title, f"{safe_title}.json")
        docx_path = os.path.join(safe_title, f"{safe_title}.docx")
        
        self._save_txt(chapters_data, txt_path)
        self._save_json(chapters_data, json_path)
        
        if DOCX_AVAILABLE:
            self._save_word(chapters_data, docx_path)
        
        return safe_title
    
    def _save_txt(self, chapters_data, filename):
        """ä¿å­˜TXTæ–‡ä»¶"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(f"{self.book_info['title']}\n")
                f.write("=" * 60 + "\n")
                f.write(f"ä½œè€…ï¼š{self.book_info['author']}\n")
                f.write("çˆ¬å–æ—¶é—´ï¼š" + time.strftime('%Y-%m-%d %H:%M:%S') + "\n")
                f.write(f"æ¥æºï¼š{self.catalog_url}\n\n")
                
                for chapter in chapters_data:
                    f.write(f"{chapter['title']}\n")
                    f.write("-" * 50 + "\n\n")
                    if chapter['content']:
                        f.write(chapter['content'])
                    else:
                        f.write("[æ­¤ç« èŠ‚å†…å®¹è·å–å¤±è´¥]")
                    f.write(f"\n\n\n")
            
            print(f"âœ… TXTæ–‡ä»¶å·²ä¿å­˜: {filename}")
        except Exception as e:
            print(f"âŒ ä¿å­˜TXTå¤±è´¥: {e}")
    
    def _save_json(self, chapters_data, filename):
        """ä¿å­˜JSONæ–‡ä»¶"""
        try:
            data = {
                'title': self.book_info['title'],
                'author': self.book_info['author'],
                'crawl_time': time.strftime('%Y-%m-%d %H:%M:%S'),
                'source_url': self.catalog_url,
                'chapters': chapters_data
            }
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… JSONæ–‡ä»¶å·²ä¿å­˜: {filename}")
        except Exception as e:
            print(f"âŒ ä¿å­˜JSONå¤±è´¥: {e}")
    
    def _save_word(self, chapters_data, filename):
        """ä¿å­˜Wordæ–‡æ¡£"""
        try:
            document = Document()
            
            # è®¾ç½®å­—ä½“
            document.styles['Normal'].font.name = 'æ¥·ä½“'
            document.styles['Normal']._element.rPr.rFonts.set(qn('w:eastAsia'), 'æ¥·ä½“')
            document.styles['Normal'].font.size = Pt(14)
            
            # æ–‡æ¡£æ ‡é¢˜
            title = document.add_heading(self.book_info['title'], 0)
            title.runs[0].font.name = 'æ¥·ä½“'
            title.runs[0]._element.rPr.rFonts.set(qn('w:eastAsia'), 'æ¥·ä½“')
            title.runs[0].font.size = Pt(18)

        
            
            # ä½œè€…ä¿¡æ¯
            author_para = document.add_paragraph()
            author_run = author_para.add_run(f'ä½œè€…ï¼š{self.book_info["author"]}')
            author_run.font.name = 'æ¥·ä½“'
            author_run._element.rPr.rFonts.set(qn('w:eastAsia'), 'æ¥·ä½“')
            author_run.font.size = Pt(14)
           
            
            document.add_page_break()
            
            # ç« èŠ‚å†…å®¹
            for i, chapter in enumerate(chapters_data):
                if not chapter.get('success', False) or not chapter.get('content'):
                    continue
                
                # ç« èŠ‚æ ‡é¢˜
                title_heading = document.add_heading(chapter['title'], 1)
                title_heading.alignment = WD_ALIGN_PARAGRAPH.CENTER
                title_heading.runs[0].font.name = 'æ¥·ä½“'
                title_heading.runs[0]._element.rPr.rFonts.set(qn('w:eastAsia'), 'æ¥·ä½“')
                title_heading.runs[0].font.size = Pt(18)
                title_heading.runs[0].bold = True
                title_heading.runs[0].font.color.rgb = RGBColor(0, 0, 0)  # è®¾ç½®ä¸ºé»‘è‰²
                title_heading.paragraph_format.space_after = Pt(12)  # ç« èŠ‚æ ‡é¢˜åå¢åŠ é—´è·
                
                # ç« èŠ‚å†…å®¹
                paragraphs = chapter['content'].split('\n\n')
                for para_text in paragraphs:
                    para_text = para_text.strip()
                    if para_text:
                        para = document.add_paragraph()
                        para_run = para.add_run(para_text)
                        para_run.font.name = 'æ¥·ä½“'
                        para_run._element.rPr.rFonts.set(qn('w:eastAsia'), 'æ¥·ä½“')
                        para_run.font.size = Pt(12)
                        
                        para.paragraph_format.first_line_indent = Pt(24)
                        para.paragraph_format.space_after = Pt(8)
                        para.paragraph_format.line_spacing = 1.15
                        para.alignment = WD_ALIGN_PARAGRAPH.LEFT
                
                # æ¯ç« åˆ†é¡µï¼ˆé™¤æœ€åä¸€ç« ï¼‰
                if i < len([ch for ch in chapters_data if ch.get('success', False)]) - 1:
                    document.add_page_break()
            
            document.save(filename)
            print(f"âœ… Wordæ–‡æ¡£å·²ä¿å­˜: {filename}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜Wordå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("        é€šç”¨å°è¯´çˆ¬è™« v3.1 - ä¼˜åŒ–ç‰ˆ")
    print("=" * 60)
    print("ğŸ¯ æ”¯æŒå¤§éƒ¨åˆ†å°è¯´ç½‘ç«™ç›®å½•é¡µé¢")
    print("ğŸ”— æ™ºèƒ½åˆå¹¶æ ‡é¢˜ï¼Œä¸¥æ ¼æŒ‰pæ ‡ç­¾åˆ†æ®µ")
    print("ğŸ“„ è‡ªåŠ¨ç”ŸæˆTXTã€JSONã€Wordä¸‰ç§æ ¼å¼")
    print("âš ï¸  è¯·éµå®ˆç½‘ç«™ä½¿ç”¨æ¡æ¬¾")
    print("=" * 60)
    
    # è·å–URL
    while True:
        catalog_url = input("\nğŸ“ è¯·è¾“å…¥å°è¯´ç›®å½•é¡µé¢URL: ").strip()
        
        if not catalog_url:
            print("âŒ URLä¸èƒ½ä¸ºç©º")
            continue
        
        if not catalog_url.startswith(('http://', 'https://')):
            print("âŒ è¯·è¾“å…¥å®Œæ•´URL")
            continue
        
        # æµ‹è¯•URL
        try:
            test_response = requests.get(catalog_url, timeout=10)
            if test_response.status_code == 200:
                print("âœ… URLå¯è®¿é—®")
                break
            else:
                print(f"âš ï¸  è¿”å›çŠ¶æ€ç  {test_response.status_code}")
                if input("ç»§ç»­ä½¿ç”¨æ­¤URLï¼Ÿ(y/n): ").lower() in ['y', 'yes']:
                    break
        except Exception as e:
            print(f"âš ï¸  URLæµ‹è¯•å¤±è´¥: {e}")
            if input("ä»è¦ä½¿ç”¨æ­¤URLï¼Ÿ(y/n): ").lower() in ['y', 'yes']:
                break
    
    crawler = UniversalNovelCrawler(catalog_url)
    
    # é€‰æ‹©æ¨¡å¼
    while True:
        print("\nè¯·é€‰æ‹©æ¨¡å¼:")
        print("1. ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼ˆå‰3ç« ï¼‰")
        print("2. ğŸš€ å®Œæ•´æ¨¡å¼ï¼ˆæ‰€æœ‰ç« èŠ‚ï¼‰")
        print("3. ğŸ“Š å…ˆæµ‹è¯•å†å†³å®šï¼ˆæ¨èï¼‰")
        print("4. âŒ é€€å‡º")
        
        choice = input("é€‰æ‹© (1/2/3/4): ").strip()
        
        if choice == "1":
            crawler.crawl_book(delay=2, test_mode=True)
            break
        elif choice == "2":
            if input("ç¡®è®¤çˆ¬å–æ‰€æœ‰ç« èŠ‚ï¼Ÿ(y/n): ").lower() in ['y', 'yes']:
                crawler.crawl_book(delay=3, test_mode=False)
                break
        elif choice == "3":
            print("ğŸ“Š å…ˆè¿›è¡Œæµ‹è¯•...")
            test_folder = crawler.crawl_book(delay=2, test_mode=True)
            if test_folder and input("æ»¡æ„å—ï¼Ÿç»§ç»­çˆ¬å–å®Œæ•´ç‰ˆï¼Ÿ(y/n): ").lower() in ['y', 'yes']:
                crawler.crawl_book(delay=3, test_mode=False)
            break
        elif choice == "4":
            print("ğŸ‘‹ å†è§ï¼")
            break
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")

if __name__ == "__main__":
    main()