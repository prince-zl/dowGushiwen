import requests
from bs4 import BeautifulSoup
import time
import os
import json
import re
from urllib.parse import urljoin

class JigongCrawler:
    def __init__(self):
        self.base_url = "https://m.gushiwen.cn"  # ä½¿ç”¨ç§»åŠ¨ç‰ˆï¼Œæ›´ç¨³å®š
        self.desktop_url = "https://www.gushiwen.cn"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Connection': 'keep-alive',
            'Referer': 'https://m.gushiwen.cn/',
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
    def generate_known_chapters(self):
        """åŸºäºæœç´¢ç»“æœç”Ÿæˆå·²çŸ¥å­˜åœ¨çš„ç« èŠ‚"""
        chapters = []
        
        # ä»æœç´¢ç»“æœä¸­è·å¾—çš„ç¡®å®å­˜åœ¨çš„ç« èŠ‚
        known_chapters = [
            # åŸºäºæœç´¢ç»“æœçš„çœŸå®URL
            ("ç¬¬ä¸€ç™¾å…­åä¸€å› é€›è¥¿æ¹–æ¶éœ¸é‡å¦–é£ çœ‹åˆè¯­ç§è®¿ç™½é±¼å¯º", 
             "https://m.gushiwen.cn/guwen/bookv_098fbba030de.aspx"),
        ]
        
        # æ·»åŠ å·²çŸ¥ç« èŠ‚
        for title, url in known_chapters:
            chapters.append({'title': title, 'url': url})
        
        print(f"âœ… å‡†å¤‡äº† {len(chapters)} ä¸ªç¡®è®¤å­˜åœ¨çš„ç« èŠ‚")
        return chapters
    
    def try_find_chapter_pattern(self):
        """å°è¯•å‘ç°ç« èŠ‚URLè§„å¾‹"""
        print("ğŸ” åˆ†æç½‘ç«™ç« èŠ‚URLæ¨¡å¼...")
        
        # ç”±äºæˆ‘ä»¬çŸ¥é“ä¸»é¡µé¢æœ‰ç« èŠ‚é“¾æ¥ï¼Œå°è¯•ä¸åŒçš„æ–¹æ³•è·å–
        main_page_urls = [
            "https://www.gushiwen.cn/guwen/book_ce3ab505d8e6.aspx",
            "https://m.gushiwen.cn/guwen/book_ce3ab505d8e6.aspx"
        ]
        
        chapters = []
        
        for url in main_page_urls:
            try:
                print(f"å°è¯•åˆ†æ: {url}")
                response = self.session.get(url, timeout=15)
                
                if response.status_code == 200:
                    # æ£€æŸ¥æ˜¯å¦æœ‰JavaScriptåŠ¨æ€åŠ è½½çš„å†…å®¹
                    if 'bookv_' in response.text:
                        print("âœ… å‘ç°bookv_æ¨¡å¼çš„é“¾æ¥")
                        
                        # ä½¿ç”¨æ­£åˆ™æå–æ‰€æœ‰å¯èƒ½çš„ç« èŠ‚ID
                        pattern = r'bookv_([a-f0-9]{12})\.aspx'
                        matches = re.findall(pattern, response.text)
                        
                        # ä¸é™åˆ¶æ•°é‡ï¼Œå‘ç°æ‰€æœ‰ç« èŠ‚
                        for i, chapter_id in enumerate(matches, 1):
                            chapter_url = f"https://m.gushiwen.cn/guwen/bookv_{chapter_id}.aspx"
                            chapters.append({
                                'title': f'ç¬¬{i}å›',
                                'url': chapter_url
                            })
                            print(f"  å‘ç°ç« èŠ‚: bookv_{chapter_id}.aspx")
                
            except Exception as e:
                print(f"åˆ†æ {url} å¤±è´¥: {e}")
                continue
        
        if chapters:
            print(f"ğŸ¯ é€šè¿‡æ¨¡å¼åˆ†æå‘ç° {len(chapters)} ä¸ªç« èŠ‚")
        else:
            print("âš ï¸  æ— æ³•é€šè¿‡æ¨¡å¼åˆ†æå‘ç°ç« èŠ‚")
            
        return chapters
    
    def get_chapter_content(self, chapter_url, chapter_title):
        """è·å–å•ä¸ªç« èŠ‚å†…å®¹ - ä¸¥æ ¼æŒ‰ç…§pæ ‡ç­¾åˆ†æ®µ"""
        try:
            print(f"  ğŸ“– è·å–: {chapter_title}")
            
            response = self.session.get(chapter_url, timeout=15)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                print(f"    âŒ HTTPé”™è¯¯: {response.status_code}")
                return ""
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                element.decompose()
            
            content = ""
            
            # ç­–ç•¥1: ä¸¥æ ¼æŒ‰pæ ‡ç­¾åˆ†æ®µ - é’ˆå¯¹å¤è¯—æ–‡ç½‘çš„ç‰¹å®šç»“æ„
            gushiwen_selectors = [
                '.main3 .left .cont',  # å¤è¯—æ–‡ç½‘æ¡Œé¢ç‰ˆå¸¸ç”¨ç»“æ„
                '.main3 .cont',        # ç®€åŒ–ç‰ˆ
                '.cont',               # ä¸»å†…å®¹åŒº
                '[class*="main"] .cont',
                '#main .cont',
                '.left .cont',
                '.content',
                '.main-content'
            ]
            
            for selector in gushiwen_selectors:
                content_container = soup.select_one(selector)
                if content_container:
                    print(f"    âœ… æ‰¾åˆ°å†…å®¹å®¹å™¨: {selector}")
                    
                    # ä¸¥æ ¼æå–æ‰€æœ‰pæ ‡ç­¾ä½œä¸ºæ®µè½
                    paragraphs = content_container.find_all('p')
                    
                    if paragraphs:
                        paragraph_texts = []
                        for p in paragraphs:
                            p_text = p.get_text(strip=True)
                            if p_text and len(p_text) > 5:  # è¿‡æ»¤å¤ªçŸ­çš„æ®µè½
                                paragraph_texts.append(p_text)
                        
                        if paragraph_texts:
                            content = '\n\n'.join(paragraph_texts)  # æ¯ä¸ªpæ ‡ç­¾é—´ç”¨åŒæ¢è¡Œåˆ†éš”
                            print(f"    âœ… ä¸¥æ ¼æŒ‰pæ ‡ç­¾æå–åˆ° {len(paragraph_texts)} ä¸ªæ®µè½")
                            break
                    
                    # å¦‚æœå®¹å™¨å†…æ²¡æœ‰pæ ‡ç­¾ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–æ®µè½æ ‡è®°
                    if not content:
                        # æ£€æŸ¥æ˜¯å¦æœ‰divæˆ–spanä½œä¸ºæ®µè½åˆ†éš”
                        div_paragraphs = content_container.find_all(['div', 'span'])
                        if div_paragraphs and len(div_paragraphs) > 1:
                            para_texts = []
                            for div in div_paragraphs:
                                div_text = div.get_text(strip=True)
                                if div_text and len(div_text) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„å†…å®¹
                                    para_texts.append(div_text)
                            
                            if para_texts and len(para_texts) > 1:
                                content = '\n\n'.join(para_texts)
                                print(f"    âœ… æŒ‰div/spanæ ‡ç­¾æå–åˆ° {len(para_texts)} ä¸ªæ®µè½")
                                break
                        
                        # å¦‚æœéƒ½æ²¡æœ‰ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰bræ ‡ç­¾åˆ†å‰²çš„å†…å®¹
                        container_html = str(content_container)
                        if '<br' in container_html.lower():
                            # å°†bræ ‡ç­¾æ›¿æ¢ä¸ºæ®µè½åˆ†éš”ç¬¦
                            br_separated = re.sub(r'<br[^>]*?/?>', '\n||PARAGRAPH_BREAK||\n', container_html)
                            # ç§»é™¤å…¶ä»–HTMLæ ‡ç­¾
                            clean_text = BeautifulSoup(br_separated, 'html.parser').get_text()
                            # æŒ‰æ®µè½åˆ†éš”ç¬¦åˆ†æ®µ
                            paragraphs = [p.strip() for p in clean_text.split('||PARAGRAPH_BREAK||')]
                            paragraphs = [p for p in paragraphs if p and len(p) > 10]
                            
                            if paragraphs:
                                content = '\n\n'.join(paragraphs)
                                print(f"    âœ… æŒ‰bræ ‡ç­¾åˆ†æ®µæå–åˆ° {len(paragraphs)} ä¸ªæ®µè½")
                                break
            
            # ç­–ç•¥2: å¦‚æœå®¹å™¨ç­–ç•¥å¤±è´¥ï¼Œç›´æ¥åœ¨æ•´ä¸ªé¡µé¢ä¸­æŸ¥æ‰¾æ‰€æœ‰pæ ‡ç­¾
            if not content or len(content) < 200:
                print(f"    ğŸ”§ ç­–ç•¥1å¤±è´¥ï¼Œåœ¨æ•´ä¸ªé¡µé¢æŸ¥æ‰¾pæ ‡ç­¾...")
                
                all_paragraphs = soup.find_all('p')
                
                if all_paragraphs:
                    paragraph_texts = []
                    for p in all_paragraphs:
                        p_text = p.get_text(strip=True)
                        # è¿‡æ»¤æ‰æ˜æ˜¾çš„å¯¼èˆªã€èœå•ã€ç‰ˆæƒä¿¡æ¯
                        if (p_text and len(p_text) > 15 and 
                            not any(skip in p_text.lower() for skip in 
                                   ['å¯¼èˆª', 'èœå•', 'ç™»å½•', 'æ³¨å†Œ', 'é¦–é¡µ', 'ç‰ˆæƒ', 'copyright', 
                                    'å…³äºæˆ‘ä»¬', 'è”ç³»æˆ‘ä»¬', 'ç”¨æˆ·åè®®', 'éšç§æ”¿ç­–', 'æ„è§åé¦ˆ'])):
                            paragraph_texts.append(p_text)
                    
                    if paragraph_texts and len(paragraph_texts) > 2:
                        content = '\n\n'.join(paragraph_texts)
                        print(f"    âœ… ä»å…¨é¡µé¢ä¸¥æ ¼æŒ‰pæ ‡ç­¾æå–åˆ° {len(paragraph_texts)} ä¸ªæ®µè½")
            
            # ç­–ç•¥3: æŸ¥æ‰¾å…·æœ‰æ˜ç¡®æ®µè½ç»“æ„çš„å¤§æ–‡æœ¬å—
            if not content or len(content) < 100:
                print(f"    ğŸ”§ ç­–ç•¥2å¤±è´¥ï¼ŒæŸ¥æ‰¾ç»“æ„åŒ–æ–‡æœ¬å—...")
                
                # æŸ¥æ‰¾å¯èƒ½åŒ…å«æ•…äº‹å†…å®¹çš„å…ƒç´ 
                content_elements = soup.find_all(['div', 'article', 'section', 'main'])
                
                best_content = ""
                best_paragraph_count = 0
                
                for elem in content_elements:
                    # æ£€æŸ¥è¯¥å…ƒç´ å†…çš„æ®µè½ç»“æ„
                    elem_paragraphs = elem.find_all('p')
                    
                    if elem_paragraphs and len(elem_paragraphs) >= 3:  # è‡³å°‘3ä¸ªæ®µè½æ‰è€ƒè™‘
                        para_texts = []
                        for p in elem_paragraphs:
                            p_text = p.get_text(strip=True)
                            if p_text and len(p_text) > 10:
                                para_texts.append(p_text)
                        
                        # è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯æ­£æ–‡çš„å†…å®¹
                        filtered_paras = []
                        for para in para_texts:
                            if not any(skip in para.lower() for skip in 
                                     ['å¯¼èˆª', 'èœå•', 'ç™»å½•', 'æ³¨å†Œ', 'é¦–é¡µ', 'javascript:', 
                                      'ç‰ˆæƒæ‰€æœ‰', 'å…³äºæˆ‘ä»¬', 'è”ç³»æˆ‘ä»¬', 'ç”¨æˆ·åè®®']):
                                filtered_paras.append(para)
                        
                        if len(filtered_paras) > best_paragraph_count and len(filtered_paras) >= 3:
                            best_content = '\n\n'.join(filtered_paras)
                            best_paragraph_count = len(filtered_paras)
                
                if best_content:
                    content = best_content
                    print(f"    âœ… æ‰¾åˆ°ç»“æ„åŒ–å†…å®¹å—ï¼Œä¸¥æ ¼æŒ‰æ®µè½åˆ†å‰² ({best_paragraph_count} ä¸ªæ®µè½)")
            
            # æœ€ç»ˆå†…å®¹éªŒè¯å’Œæ ¼å¼åŒ–
            if content:
                # æ¸…ç†å¤šä½™çš„ç©ºè¡Œï¼Œä½†ä¿æŒåŒæ¢è¡Œçš„æ®µè½åˆ†éš”
                content = re.sub(r'\n\s*\n\s*\n+', '\n\n', content)
                content = content.strip()
                
                # éªŒè¯å†…å®¹è´¨é‡
                paragraph_count = content.count('\n\n') + 1
                
                if len(content) > 100 and paragraph_count >= 2:
                    print(f"    âœ… æœ€ç»ˆæˆåŠŸ ({len(content)} å­—ç¬¦, {paragraph_count} ä¸ªæ®µè½)")
                    print(f"    ğŸ“‹ æ®µè½é¢„è§ˆ: {content[:100]}...")
                    return content
                else:
                    print(f"    âš ï¸  å†…å®¹è´¨é‡ä¸è¶³ ({len(content)} å­—ç¬¦, {paragraph_count} ä¸ªæ®µè½)")
            
            print(f"    âŒ æ‰€æœ‰ç­–ç•¥éƒ½æœªèƒ½æå–åˆ°æœ‰æ•ˆçš„åˆ†æ®µå†…å®¹")
            
            # è°ƒè¯•ä¿¡æ¯
            print(f"    ğŸ” è°ƒè¯•ä¿¡æ¯:")
            print(f"       é¡µé¢æ ‡é¢˜: {soup.title.string if soup.title else 'æ— æ ‡é¢˜'}")
            print(f"       é¡µé¢å¤§å°: {len(response.text)} å­—ç¬¦")
            print(f"       pæ ‡ç­¾æ•°é‡: {len(soup.find_all('p'))}")
            print(f"       æ˜¯å¦åŒ…å«'è¯è¯´': {'è¯è¯´' in response.text}")
            print(f"       æ˜¯å¦åŒ…å«'æµå…¬': {'æµå…¬' in response.text}")
            
            return ""
            
        except Exception as e:
            print(f"    âŒ è·å–å¤±è´¥: {e}")
            return ""
    
    def crawl_book(self, delay=3, test_mode=False):
        """çˆ¬å–æ•´æœ¬ä¹¦"""
        print("ğŸš€ å¼€å§‹çˆ¬å–ã€Šæµå…¬å…¨ä¼ ã€‹...")
        print("=" * 60)
        
        # è·å–ç« èŠ‚åˆ—è¡¨
        chapters = []
        
        # æ–¹æ³•1: å°è¯•æ¨¡å¼åˆ†æ
        pattern_chapters = self.try_find_chapter_pattern()
        chapters.extend(pattern_chapters)
        
        # æ–¹æ³•2: ä½¿ç”¨å·²çŸ¥ç« èŠ‚
        known_chapters = self.generate_known_chapters()
        chapters.extend(known_chapters)
        
        # å»é™¤é‡å¤
        unique_chapters = []
        seen_urls = set()
        for chapter in chapters:
            if chapter['url'] not in seen_urls:
                unique_chapters.append(chapter)
                seen_urls.add(chapter['url'])
        
        chapters = unique_chapters
        
        if not chapters:
            print("âŒ æ— æ³•è·å–ä»»ä½•ç« èŠ‚ä¿¡æ¯")
            return
        
        # æµ‹è¯•æ¨¡å¼ï¼šåªçˆ¬å–å‰å‡ ç« 
        if test_mode:
            chapters = chapters[:3]
            print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šåªçˆ¬å–å‰ {len(chapters)} ç« ")
        
        print(f"ğŸ“š æ‰¾åˆ° {len(chapters)} ä¸ªç« èŠ‚ï¼Œå¼€å§‹çˆ¬å–...")
        print("-" * 60)
        
        # çˆ¬å–ç« èŠ‚å†…å®¹
        chapters_data = []
        success_count = 0
        
        for i, chapter in enumerate(chapters, 1):
            print(f"[{i:3d}/{len(chapters)}] {chapter['title']}")
            
            content = self.get_chapter_content(chapter['url'], chapter['title'])
            
            # è®¡ç®—æ®µè½æ•°é‡
            paragraph_count = content.count('\n\n') + 1 if content else 0
            
            chapter_data = {
                'title': chapter['title'],
                'url': chapter['url'],
                'content': content,
                'char_count': len(content),
                'paragraph_count': paragraph_count,
                'success': len(content) > 100 and paragraph_count >= 2
            }
            
            chapters_data.append(chapter_data)
            
            if chapter_data['success']:
                success_count += 1
                print(f"           âœ… æˆåŠŸ ({paragraph_count} ä¸ªæ®µè½)")
            else:
                print(f"           âŒ å¤±è´¥æˆ–å†…å®¹ä¸å®Œæ•´")
            
            time.sleep(delay)
        
        # ä¿å­˜ç»“æœ
        print("-" * 60)
        print(f"ğŸ“‹ çˆ¬å–å®Œæˆ:")
        print(f"   æ€»ç« èŠ‚: {len(chapters_data)}")
        print(f"   æˆåŠŸ: {success_count}")
        print(f"   å¤±è´¥: {len(chapters_data) - success_count}")
        
        # ç»Ÿè®¡æ®µè½ä¿¡æ¯
        total_paragraphs = sum(ch.get('paragraph_count', 0) for ch in chapters_data)
        print(f"   æ€»æ®µè½æ•°: {total_paragraphs}")
        
        if success_count > 0:
            self.save_to_file(chapters_data)
            self.save_to_json(chapters_data)
            print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼è·å¾— {success_count} ä¸ªæœ‰æ•ˆç« èŠ‚ï¼Œå…± {total_paragraphs} ä¸ªæ®µè½")
        else:
            print("âŒ æ²¡æœ‰æˆåŠŸè·å–ä»»ä½•ç« èŠ‚å†…å®¹")
            print("\nğŸ’¡ å»ºè®®:")
            print("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            print("   2. ç¨åé‡è¯•ï¼ˆå¯èƒ½é‡åˆ°é¢‘ç‡é™åˆ¶ï¼‰")
            print("   3. å°è¯•ä½¿ç”¨VPNæˆ–æ›´æ¢IP")
    
    def save_to_file(self, chapters_data, filename="æµå…¬å…¨ä¼ .txt"):
        """ä¿å­˜å†…å®¹åˆ°æ–‡ä»¶ - ä¿æŒæ®µè½æ ¼å¼"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write("æµå…¬å…¨ä¼ \n")
                f.write("=" * 60 + "\n")
                f.write("ä½œè€…ï¼šéƒ­å°äº­ï¼ˆæ¸…ä»£ï¼‰\n")
                f.write("çˆ¬å–æ—¶é—´ï¼š" + time.strftime('%Y-%m-%d %H:%M:%S') + "\n")
                f.write("è¯´æ˜ï¼šä¸¥æ ¼æŒ‰ç…§åŸç½‘ç«™pæ ‡ç­¾åˆ†æ®µä¿å­˜\n")
                f.write("=" * 60 + "\n\n")
                
                total_chars = 0
                total_paragraphs = 0
                
                for i, chapter in enumerate(chapters_data, 1):
                    f.write(f"{chapter['title']}\n")
                    f.write("-" * 50 + "\n\n")
                    
                    if chapter['content']:
                        f.write(chapter['content'])  # å†…å®¹å·²ç»æŒ‰æ®µè½æ ¼å¼åŒ–ï¼Œç›´æ¥å†™å…¥
                        total_chars += len(chapter['content'])
                        total_paragraphs += chapter.get('paragraph_count', 0)
                    else:
                        f.write("[æ­¤ç« èŠ‚å†…å®¹è·å–å¤±è´¥]")
                    
                    f.write(f"\n\n\n\n")
                    # f.write(f"\næ¥æº: {chapter['url']}")
                    # f.write("\n\n" + "=" * 50 + "\n\n")
                
                f.write(f"\næ€»è®¡: {len(chapters_data)} ç« , {total_chars} å­—, {total_paragraphs} æ®µè½\n")
            
            print(f"âœ… å†…å®¹å·²ä¿å­˜åˆ°: {filename} (ä¿æŒåŸå§‹æ®µè½æ ¼å¼)")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
    
    def save_to_json(self, chapters_data, filename="æµå…¬å…¨ä¼ .json"):
        """ä¿å­˜ä¸ºJSONæ ¼å¼"""
        try:
            data = {
                'title': 'æµå…¬å…¨ä¼ ',
                'author': 'éƒ­å°äº­',
                'crawl_time': time.strftime('%Y-%m-%d %H:%M:%S'),
                'format_note': 'ä¸¥æ ¼æŒ‰ç…§åŸç½‘ç«™pæ ‡ç­¾åˆ†æ®µ',
                'total_chapters': len(chapters_data),
                'success_chapters': len([ch for ch in chapters_data if ch.get('success', False)]),
                'total_chars': sum(ch['char_count'] for ch in chapters_data),
                'total_paragraphs': sum(ch.get('paragraph_count', 0) for ch in chapters_data),
                'chapters': chapters_data
            }
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… JSONæ•°æ®å·²ä¿å­˜åˆ°: {filename}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜JSONå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("        æµå…¬å…¨ä¼  - ä¸“ä¸šç‰ˆçˆ¬è™« v2.1")
    print("        (ä¸¥æ ¼æŒ‰pæ ‡ç­¾åˆ†æ®µç‰ˆæœ¬)")
    print("=" * 60)
    print("ğŸ¯ é’ˆå¯¹å¤è¯—æ–‡ç½‘ä¼˜åŒ–ï¼Œä¸¥æ ¼æŒ‰pæ ‡ç­¾åˆ†æ®µ")
    print("âš ï¸  è¯·éµå®ˆç½‘ç«™ä½¿ç”¨æ¡æ¬¾ï¼Œä»…ç”¨äºå­¦ä¹ ç ”ç©¶")
    print("=" * 60)
    
    crawler = JigongCrawler()
    
    # è¯¢é—®ç”¨æˆ·æƒ³è¦çš„æ¨¡å¼
    print("è¯·é€‰æ‹©çˆ¬å–æ¨¡å¼:")
    print("1. ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼ˆåªçˆ¬å–3ç« ï¼Œå¿«é€ŸéªŒè¯åˆ†æ®µæ•ˆæœï¼‰")
    print("2. ğŸš€ å®Œæ•´æ¨¡å¼ï¼ˆçˆ¬å–æ‰€æœ‰ç« èŠ‚ï¼Œä¸¥æ ¼åˆ†æ®µï¼‰")
    print("3. ğŸ“Š å…ˆæµ‹è¯•å†å†³å®šï¼ˆæ¨èï¼‰")
    
    while True:
        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1/2/3): ").strip()
        
        if choice == "1":
            print("\nğŸ§ª å¼€å§‹æµ‹è¯•æ¨¡å¼...")
            crawler.crawl_book(delay=2, test_mode=True)
            break
            
        elif choice == "2":
            print("\nğŸš€ å¼€å§‹å®Œæ•´çˆ¬å–...")
            crawler.crawl_book(delay=3, test_mode=False)
            break
            
        elif choice == "3":
            print("\nğŸ§ª å¼€å§‹æµ‹è¯•æ¨¡å¼ï¼ˆå‰3ç« ï¼‰...")
            crawler.crawl_book(delay=2, test_mode=True)
            
            print("\n" + "=" * 40)
            continue_choice = input("æµ‹è¯•å®Œæˆï¼æ˜¯å¦ç»§ç»­çˆ¬å–å®Œæ•´ç‰ˆæœ¬ï¼Ÿ(y/n): ").strip().lower()
            
            if continue_choice in ['y', 'yes', 'æ˜¯']:
                print("\nğŸš€ å¼€å§‹å®Œæ•´çˆ¬å–...")
                crawler.crawl_book(delay=3, test_mode=False)
            else:
                print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼")
            break
            
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1ã€2 æˆ– 3")
    
    print(f"\nğŸ“ æ–‡ä»¶ä¿å­˜åœ¨å½“å‰ç›®å½•:")
    print(f"   ğŸ“„ æµå…¬å…¨ä¼ .txt - ä¸¥æ ¼æŒ‰æ®µè½æ ¼å¼çš„æ–‡æœ¬")
    print(f"   ğŸ“‹ æµå…¬å…¨ä¼ .json - åŒ…å«æ®µè½ç»Ÿè®¡çš„æ•°æ®æ ¼å¼")

if __name__ == "__main__":
    main()