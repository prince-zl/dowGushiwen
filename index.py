import requests
from bs4 import BeautifulSoup
import time
import os
import json
import re
from urllib.parse import urljoin, urlparse

# Wordæ–‡æ¡£ç›¸å…³å¯¼å…¥
try:
    from docx import Document
    from docx.shared import Inches, Pt
    from docx.enum.text import WD_ALIGN_PARAGRAPH
    from docx.oxml.ns import qn
    DOCX_AVAILABLE = True
    print("âœ… python-docx å·²å®‰è£…ï¼Œå°†è‡ªåŠ¨ç”ŸæˆWordæ–‡æ¡£")
except ImportError:
    DOCX_AVAILABLE = False
    print("âš ï¸  python-docx æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install python-docx")
    print("   å°†åªç”ŸæˆTXTå’ŒJSONæ–‡ä»¶")

class UniversalNovelCrawler:
    def __init__(self, catalog_url=None):
        self.catalog_url = catalog_url
        self.base_domain = self.get_base_domain(catalog_url) if catalog_url else ""
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Connection': 'keep-alive',
            'Referer': self.base_domain,
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        self.book_info = {'title': 'æœªçŸ¥ä¹¦å', 'author': 'æœªçŸ¥ä½œè€…'}
        
    def get_base_domain(self, url):
        """è·å–URLçš„åŸºç¡€åŸŸå"""
        if not url:
            return ""
        parsed = urlparse(url)
        return f"{parsed.scheme}://{parsed.netloc}"
    
    def get_book_info_from_catalog(self, soup):
        """ä»ç›®å½•é¡µé¢æå–ä¹¦åå’Œä½œè€…ä¿¡æ¯"""
        try:
            # å¸¸è§çš„ä¹¦åé€‰æ‹©å™¨
            title_selectors = [
                'h1', 'h2', '.book-title', '.title', '#title',
                '.book-name', '.bookname', '.novel-title',
                '[class*="title"]', '[id*="title"]',
                '.main h1', '.content h1', '.header h1'
            ]
            
            book_title = "æœªçŸ¥ä¹¦å"
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    title_text = title_elem.get_text(strip=True)
                    if title_text and len(title_text) > 0 and len(title_text) < 50:
                        book_title = title_text
                        break
            
            # å¸¸è§çš„ä½œè€…é€‰æ‹©å™¨
            author_selectors = [
                '.author', '.book-author', '#author',
                '[class*="author"]', '[id*="author"]',
                '.writer', '.novelist'
            ]
            
            book_author = "æœªçŸ¥ä½œè€…"
            for selector in author_selectors:
                author_elem = soup.select_one(selector)
                if author_elem:
                    author_text = author_elem.get_text(strip=True)
                    # æ¸…ç†ä½œè€…æ–‡æœ¬
                    author_text = re.sub(r'ä½œè€…[ï¼š:]\s*', '', author_text)
                    author_text = re.sub(r'è‘—[ï¼š:]\s*', '', author_text)
                    if author_text and len(author_text) > 0 and len(author_text) < 30:
                        book_author = author_text
                        break
            
            # å¦‚æœåœ¨æ ‡é¢˜ä¸­æ‰¾åˆ°ä½œè€…ä¿¡æ¯
            if "ä½œè€…" in book_title:
                parts = re.split(r'[ä½œè€…ï¼š:]', book_title)
                if len(parts) >= 2:
                    book_title = parts[0].strip()
                    book_author = parts[1].strip()
            
            self.book_info = {
                'title': book_title,
                'author': book_author
            }
            
            print(f"ğŸ“š æ£€æµ‹åˆ°ä¹¦ç±ä¿¡æ¯:")
            print(f"   ä¹¦å: {book_title}")
            print(f"   ä½œè€…: {book_author}")
            
        except Exception as e:
            print(f"âš ï¸  è·å–ä¹¦ç±ä¿¡æ¯å¤±è´¥: {e}")
            self.book_info = {'title': 'æœªçŸ¥ä¹¦å', 'author': 'æœªçŸ¥ä½œè€…'}
    
    def parse_chapter_list(self):
        """ä»ç›®å½•é¡µé¢è§£æç« èŠ‚åˆ—è¡¨"""
        if not self.catalog_url:
            print("âŒ æœªè®¾ç½®ç›®å½•URL")
            return []
        
        print(f"ğŸ” æ­£åœ¨è§£æç›®å½•é¡µé¢: {self.catalog_url}")
        
        try:
            response = self.session.get(self.catalog_url, timeout=15)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                print(f"âŒ ç›®å½•é¡µé¢è®¿é—®å¤±è´¥: {response.status_code}")
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–ä¹¦ç±ä¿¡æ¯
            self.get_book_info_from_catalog(soup)
            
            chapters = []
            
            # ç­–ç•¥1: å¯»æ‰¾åŒ…å«ç« èŠ‚é“¾æ¥çš„å®¹å™¨
            container_selectors = [
                '.chapter-list', '.catalogue', '.catalog', '.list',
                '.content-list', '.book-list', '#list', '#catalog',
                '.main .list', '.content .list', '[class*="chapter"]',
                '[class*="catalog"]', '[class*="list"]', 'ul', 'ol',
                '.directory', '.index', '.toc'
            ]
            
            chapter_container = None
            for selector in container_selectors:
                container = soup.select_one(selector)
                if container:
                    # æ£€æŸ¥å®¹å™¨å†…æ˜¯å¦æœ‰è¶³å¤Ÿå¤šçš„é“¾æ¥
                    links = container.find_all('a', href=True)
                    if len(links) >= 3:  # è‡³å°‘3ä¸ªé“¾æ¥æ‰è®¤ä¸ºæ˜¯ç« èŠ‚å®¹å™¨
                        chapter_container = container
                        print(f"âœ… æ‰¾åˆ°ç« èŠ‚å®¹å™¨: {selector} (åŒ…å« {len(links)} ä¸ªé“¾æ¥)")
                        break
            
            if chapter_container:
                links = chapter_container.find_all('a', href=True)
                
                for link in links:
                    href = link.get('href')
                    title = link.get_text(strip=True)
                    
                    # è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯ç« èŠ‚çš„é“¾æ¥
                    if (title and len(title) > 0 and len(title) < 100 and
                        not any(skip in title.lower() for skip in 
                               ['é¦–é¡µ', 'ä¹¦æ¶', 'ç™»å½•', 'æ³¨å†Œ', 'æœç´¢', 'æ’è¡Œ', 
                                'home', 'login', 'register', 'search', 'rank',
                                'ä¸Šä¸€é¡µ', 'ä¸‹ä¸€é¡µ', 'è¿”å›', 'ç›®å½•'])):
                        
                        # å°†ç›¸å¯¹URLè½¬æ¢ä¸ºç»å¯¹URL
                        full_url = urljoin(self.catalog_url, href)
                        
                        chapters.append({
                            'title': title,
                            'url': full_url
                        })
            
            # ç­–ç•¥2: å¦‚æœæ²¡æ‰¾åˆ°å®¹å™¨ï¼Œç›´æ¥åœ¨æ•´ä¸ªé¡µé¢æŸ¥æ‰¾é“¾æ¥
            if not chapters:
                print("ğŸ”§ ç­–ç•¥1å¤±è´¥ï¼Œå°è¯•åœ¨æ•´ä¸ªé¡µé¢æŸ¥æ‰¾ç« èŠ‚é“¾æ¥...")
                
                all_links = soup.find_all('a', href=True)
                
                # åˆ†æé“¾æ¥æ¨¡å¼ï¼Œæ‰¾å‡ºå¯èƒ½çš„ç« èŠ‚é“¾æ¥
                potential_chapters = []
                
                for link in all_links:
                    href = link.get('href')
                    title = link.get_text(strip=True)
                    
                    # æ£€æŸ¥é“¾æ¥æ–‡æœ¬æ˜¯å¦åƒç« èŠ‚æ ‡é¢˜
                    if (title and len(title) > 3 and len(title) < 100 and
                        (re.search(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+[ç« å›èŠ‚å·é›†éƒ¨]', title) or
                         re.search(r'[ç¬¬]?\d+[ç« å›èŠ‚å·é›†éƒ¨]', title) or
                         re.search(r'chapter\s*\d+', title, re.I) or
                         'ç« ' in title or 'å›' in title or 'èŠ‚' in title)):
                        
                        full_url = urljoin(self.catalog_url, href)
                        potential_chapters.append({
                            'title': title,
                            'url': full_url
                        })
                
                # å¦‚æœæ‰¾åˆ°äº†ç–‘ä¼¼ç« èŠ‚çš„é“¾æ¥
                if potential_chapters:
                    chapters = potential_chapters
                    print(f"âœ… é€šè¿‡æ¨¡å¼åŒ¹é…æ‰¾åˆ° {len(chapters)} ä¸ªç–‘ä¼¼ç« èŠ‚")
                else:
                    # æœ€åçš„å°è¯•ï¼šæ‰¾åˆ°æœ€å¤šé“¾æ¥çš„åŒºåŸŸ
                    print("ğŸ”§ å°è¯•æœ€åç­–ç•¥ï¼šåˆ†æé“¾æ¥å¯†åº¦...")
                    
                    # æ‰¾åˆ°åŒ…å«æœ€å¤šé“¾æ¥çš„divæˆ–section
                    containers = soup.find_all(['div', 'section', 'ul', 'ol'])
                    best_container = None
                    max_links = 0
                    
                    for container in containers:
                        links_in_container = container.find_all('a', href=True)
                        if len(links_in_container) > max_links and len(links_in_container) >= 5:
                            max_links = len(links_in_container)
                            best_container = container
                    
                    if best_container:
                        links = best_container.find_all('a', href=True)
                        for link in links:
                            href = link.get('href')
                            title = link.get_text(strip=True)
                            
                            if title and len(title) > 0 and len(title) < 100:
                                full_url = urljoin(self.catalog_url, href)
                                chapters.append({
                                    'title': title,
                                    'url': full_url
                                })
                        
                        print(f"âœ… ä»æœ€ä½³å®¹å™¨æ‰¾åˆ° {len(chapters)} ä¸ªé“¾æ¥")
            
            # å»é‡
            unique_chapters = []
            seen_urls = set()
            for chapter in chapters:
                if chapter['url'] not in seen_urls:
                    unique_chapters.append(chapter)
                    seen_urls.add(chapter['url'])
            
            print(f"ğŸ“‹ æ€»å…±è§£æåˆ° {len(unique_chapters)} ä¸ªå”¯ä¸€ç« èŠ‚")
            
            # ğŸ¯ é‡è¦ï¼šå¯¹ç« èŠ‚è¿›è¡Œæ’åºï¼Œç¡®ä¿ä»ç¬¬ä¸€ç« å¼€å§‹
            if unique_chapters:
                sorted_chapters = self.sort_chapters(unique_chapters)
                print(f"ğŸ“Š ç« èŠ‚æ’åºå®Œæˆï¼Œè¯†åˆ«åˆ° {len(sorted_chapters)} ä¸ªæœ‰åºç« èŠ‚")
                unique_chapters = sorted_chapters
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªç« èŠ‚ä½œä¸ºé¢„è§ˆ
            if unique_chapters:
                print("ğŸ“– ç« èŠ‚é¢„è§ˆ:")
                for i, chapter in enumerate(unique_chapters[:5], 1):
                    print(f"   {i}. {chapter['title']}")
                if len(unique_chapters) > 5:
                    print(f"   ... è¿˜æœ‰ {len(unique_chapters) - 5} ä¸ªç« èŠ‚")
            
            return unique_chapters
            
        except Exception as e:
            print(f"âŒ è§£æç›®å½•é¡µé¢å¤±è´¥: {e}")
            return []
    
    def get_chapter_content(self, chapter_url, chapter_title):
        """è·å–å•ä¸ªç« èŠ‚å†…å®¹ - ä¸¥æ ¼æŒ‰ç…§pæ ‡ç­¾åˆ†æ®µï¼Œå¹¶æ™ºèƒ½åˆå¹¶æ ‡é¢˜"""
        try:
            print(f"  ğŸ“– è·å–: {chapter_title}")
            
            response = self.session.get(chapter_url, timeout=15)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                print(f"    âŒ HTTPé”™è¯¯: {response.status_code}")
                return {"title": chapter_title, "content": ""}
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                element.decompose()
            
            content = ""
            
            # ç­–ç•¥1: å¯»æ‰¾ä¸»è¦å†…å®¹å®¹å™¨
            content_selectors = [
                '.content', '.main-content', '.chapter-content', '.text-content',
                '#content', '#main-content', '#chapter-content', '#text-content',
                '.post-content', '.entry-content', '.article-content',
                '.main .content', '.container .content', '[class*="content"]',
                '.main3 .left .cont', '.main3 .cont', '.cont',  # å¤è¯—æ–‡ç½‘ç‰¹æœ‰
                '.chapter', '.article', '.post', '.entry'
            ]
            
            for selector in content_selectors:
                content_container = soup.select_one(selector)
                if content_container:
                    print(f"    âœ… æ‰¾åˆ°å†…å®¹å®¹å™¨: {selector}")
                    
                    # ä¸¥æ ¼æå–æ‰€æœ‰pæ ‡ç­¾ä½œä¸ºæ®µè½
                    paragraphs = content_container.find_all('p')
                    
                    if paragraphs:
                        paragraph_texts = []
                        for p in paragraphs:
                            p_text = p.get_text(strip=True)
                            if p_text and len(p_text) > 5:  # è¿‡æ»¤å¤ªçŸ­çš„æ®µè½
                                paragraph_texts.append(p_text)
                        
                        if paragraph_texts:
                            content = '\n\n'.join(paragraph_texts)  # æ¯ä¸ªpæ ‡ç­¾é—´ç”¨åŒæ¢è¡Œåˆ†éš”
                            print(f"    âœ… ä¸¥æ ¼æŒ‰pæ ‡ç­¾æå–åˆ° {len(paragraph_texts)} ä¸ªæ®µè½")
                            break
                    
                    # å¦‚æœå®¹å™¨å†…æ²¡æœ‰pæ ‡ç­¾ï¼Œæ£€æŸ¥å…¶ä»–å¯èƒ½çš„æ®µè½ç»“æ„
                    if not content:
                        # æ£€æŸ¥divæˆ–spanä½œä¸ºæ®µè½
                        div_paragraphs = content_container.find_all(['div', 'span'])
                        if len(div_paragraphs) > 1:
                            para_texts = []
                            for div in div_paragraphs:
                                div_text = div.get_text(strip=True)
                                if div_text and len(div_text) > 10:
                                    para_texts.append(div_text)
                            
                            if len(para_texts) > 1:
                                content = '\n\n'.join(para_texts)
                                print(f"    âœ… æŒ‰div/spanæ ‡ç­¾æå–åˆ° {len(para_texts)} ä¸ªæ®µè½")
                                break
                        
                        # æ£€æŸ¥bræ ‡ç­¾åˆ†å‰²çš„å†…å®¹
                        container_html = str(content_container)
                        if '<br' in container_html.lower():
                            br_separated = re.sub(r'<br[^>]*?/?>', '\n||PARAGRAPH_BREAK||\n', container_html)
                            clean_text = BeautifulSoup(br_separated, 'html.parser').get_text()
                            paragraphs = [p.strip() for p in clean_text.split('||PARAGRAPH_BREAK||')]
                            paragraphs = [p for p in paragraphs if p and len(p) > 10]
                            
                            if paragraphs:
                                content = '\n\n'.join(paragraphs)
                                print(f"    âœ… æŒ‰bræ ‡ç­¾åˆ†æ®µæå–åˆ° {len(paragraphs)} ä¸ªæ®µè½")
                                break
            
            # ç­–ç•¥2: å¦‚æœå®¹å™¨ç­–ç•¥å¤±è´¥ï¼Œç›´æ¥åœ¨æ•´ä¸ªé¡µé¢ä¸­æŸ¥æ‰¾æ‰€æœ‰pæ ‡ç­¾
            if not content or len(content) < 100:
                print(f"    ğŸ”§ ç­–ç•¥1å¤±è´¥ï¼Œåœ¨æ•´ä¸ªé¡µé¢æŸ¥æ‰¾pæ ‡ç­¾...")
                
                all_paragraphs = soup.find_all('p')
                
                if all_paragraphs:
                    paragraph_texts = []
                    for p in all_paragraphs:
                        p_text = p.get_text(strip=True)
                        # è¿‡æ»¤æ‰æ˜æ˜¾çš„å¯¼èˆªã€èœå•ã€ç‰ˆæƒä¿¡æ¯
                        if (p_text and len(p_text) > 15 and 
                            not any(skip in p_text.lower() for skip in 
                                   ['å¯¼èˆª', 'èœå•', 'ç™»å½•', 'æ³¨å†Œ', 'é¦–é¡µ', 'ç‰ˆæƒ', 'copyright', 
                                    'å…³äºæˆ‘ä»¬', 'è”ç³»æˆ‘ä»¬', 'ç”¨æˆ·åè®®', 'éšç§æ”¿ç­–', 'æ„è§åé¦ˆ',
                                    'ä¸Šä¸€ç« ', 'ä¸‹ä¸€ç« ', 'è¿”å›ç›®å½•', 'ä¹¦ç­¾', 'æ”¶è—'])):
                            paragraph_texts.append(p_text)
                    
                    if len(paragraph_texts) > 2:
                        content = '\n\n'.join(paragraph_texts)
                        print(f"    âœ… ä»å…¨é¡µé¢ä¸¥æ ¼æŒ‰pæ ‡ç­¾æå–åˆ° {len(paragraph_texts)} ä¸ªæ®µè½")
            
            # æœ€ç»ˆå†…å®¹éªŒè¯å’Œæ ¼å¼åŒ–
            if content:
                # æ¸…ç†å¤šä½™çš„ç©ºè¡Œï¼Œä½†ä¿æŒåŒæ¢è¡Œçš„æ®µè½åˆ†éš”
                content = re.sub(r'\n\s*\n\s*\n+', '\n\n', content)
                content = content.strip()
                
                # ğŸ¯ æ™ºèƒ½æ ‡é¢˜åˆå¹¶å¤„ç†
                merged_title, cleaned_content = self.merge_titles(chapter_title, content)
                
                # éªŒè¯å†…å®¹è´¨é‡
                paragraph_count = cleaned_content.count('\n\n') + 1
                
                if len(cleaned_content) > 50 and paragraph_count >= 1:
                    print(f"    âœ… æœ€ç»ˆæˆåŠŸ ({len(cleaned_content)} å­—ç¬¦, {paragraph_count} ä¸ªæ®µè½)")
                    if merged_title != chapter_title:
                        print(f"    ğŸ”— æ ‡é¢˜å·²åˆå¹¶: {merged_title}")
                    return {"title": merged_title, "content": cleaned_content}
                else:
                    print(f"    âš ï¸  å†…å®¹è´¨é‡ä¸è¶³ ({len(cleaned_content)} å­—ç¬¦, {paragraph_count} ä¸ªæ®µè½)")
                    return {"title": chapter_title, "content": cleaned_content}  # å³ä½¿è´¨é‡ä¸è¶³ä¹Ÿè¿”å›
            
            print(f"    âŒ æ‰€æœ‰ç­–ç•¥éƒ½æœªèƒ½æå–åˆ°æœ‰æ•ˆçš„åˆ†æ®µå†…å®¹")
            return {"title": chapter_title, "content": ""}
            
        except Exception as e:
            print(f"    âŒ è·å–å¤±è´¥: {e}")
            return {"title": chapter_title, "content": ""}
    
    def merge_titles(self, catalog_title, content):
        """æ™ºèƒ½åˆå¹¶ç›®å½•æ ‡é¢˜å’Œå†…å®¹æ ‡é¢˜"""
        if not content:
            return catalog_title, content
        
        # è·å–å†…å®¹çš„ç¬¬ä¸€è¡Œ - å°è¯•å¤šç§åˆ†å‰²æ–¹å¼
        lines_double = content.split('\n\n')  # åŒæ¢è¡Œåˆ†å‰²
        lines_single = content.split('\n')    # å•æ¢è¡Œåˆ†å‰²
        
        # é€‰æ‹©ç¬¬ä¸€è¡Œ
        first_line = ""
        remaining_content = content
        
        # ä¼˜å…ˆä½¿ç”¨åŒæ¢è¡Œåˆ†å‰²ï¼Œå¦‚æœç¬¬ä¸€æ®µå¤ªé•¿åˆ™ä½¿ç”¨å•æ¢è¡Œ
        if lines_double and lines_double[0].strip():
            first_line = lines_double[0].strip()
            if len(first_line) <= 150:  # å¦‚æœç¬¬ä¸€æ®µä¸å¤ªé•¿ï¼Œè®¤ä¸ºå¯èƒ½æ˜¯æ ‡é¢˜
                remaining_content = '\n\n'.join(lines_double[1:]).strip() if len(lines_double) > 1 else ""
            else:
                # ç¬¬ä¸€æ®µå¤ªé•¿ï¼Œå°è¯•å•æ¢è¡Œåˆ†å‰²
                if lines_single and lines_single[0].strip():
                    first_line = lines_single[0].strip()
                    remaining_content = '\n'.join(lines_single[1:]).strip() if len(lines_single) > 1 else ""
        elif lines_single and lines_single[0].strip():
            first_line = lines_single[0].strip()
            remaining_content = '\n'.join(lines_single[1:]).strip() if len(lines_single) > 1 else ""
        
        if not first_line:
            return catalog_title, content
        
        print(f"    ğŸ” æ£€æŸ¥ç¬¬ä¸€è¡Œ: {first_line[:50]}{'...' if len(first_line) > 50 else ''}")
        
        # åˆ¤æ–­ç¬¬ä¸€è¡Œæ˜¯å¦åƒæ ‡é¢˜
        is_title = self.is_likely_title(first_line)
        
        if is_title:
            # åˆå¹¶æ ‡é¢˜ï¼šç›®å½•æ ‡é¢˜ + å†…å®¹æ ‡é¢˜
            merged_title = self.combine_titles(catalog_title, first_line)
            
            print(f"    ğŸ”— æ£€æµ‹åˆ°å†…å®¹æ ‡é¢˜ï¼Œå·²åˆå¹¶: {first_line}")
            return merged_title, remaining_content
        else:
            # ç¬¬ä¸€è¡Œä¸æ˜¯æ ‡é¢˜ï¼Œä¿æŒåŸæ ·
            print(f"    âŒ ç¬¬ä¸€è¡Œä¸è¢«è¯†åˆ«ä¸ºæ ‡é¢˜ï¼Œä¿æŒåŸæ ·")
            return catalog_title, content
    
    def is_likely_title(self, text):
        """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦å¯èƒ½æ˜¯æ ‡é¢˜"""
        if not text:
            return False
        
        # å¤ªé•¿çš„æ–‡æœ¬ä¸å¤ªå¯èƒ½æ˜¯æ ‡é¢˜
        if len(text) > 200:
            return False
        
        # å¤ªçŸ­çš„æ–‡æœ¬ä¹Ÿä¸å¤ªå¯èƒ½æ˜¯å®Œæ•´æ ‡é¢˜
        if len(text) < 3:
            return False
        
        print(f"    ğŸ” æ ‡é¢˜åˆ¤æ–­åˆ†æ: '{text}'")
        
        # å¼ºæ ‡é¢˜ç‰¹å¾ - å¦‚æœåŒ¹é…åˆ™å‡ ä¹ç¡®å®šæ˜¯æ ‡é¢˜
        strong_title_patterns = [
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+[ç« å›èŠ‚å·é›†éƒ¨ç¯‡]',  # ç¬¬Xç« 
            r'^[ç¬¬]?\d+[ç« å›èŠ‚å·é›†éƒ¨ç¯‡]',  # Xç«  æˆ– ç¬¬Xç« 
            r'chapter\s*\d+',  # Chapter X
            r'^[\d\s\-\.]+[ç« å›èŠ‚å·é›†éƒ¨ç¯‡]',  # æ•°å­—å¼€å¤´+ç« èŠ‚è¯
            r'^ç¬¬.*[ç« å›èŠ‚å·é›†éƒ¨ç¯‡]',  # ä»¥"ç¬¬"å¼€å¤´ï¼Œä»¥ç« èŠ‚è¯ç»“å°¾
        ]
        
        for pattern in strong_title_patterns:
            if re.search(pattern, text, re.I):
                print(f"    âœ… åŒ¹é…å¼ºæ ‡é¢˜æ¨¡å¼: {pattern}")
                return True
        
        # ä¸­ç­‰æ ‡é¢˜ç‰¹å¾
        medium_indicators = 0
        
        # åŒ…å«ç« èŠ‚å…³é”®è¯
        chapter_keywords = ['ç« ', 'å›', 'èŠ‚', 'å·', 'é›†', 'éƒ¨', 'ç¯‡', 'chapter']
        if any(keyword in text.lower() for keyword in chapter_keywords):
            medium_indicators += 2
            print(f"    ğŸ“ åŒ…å«ç« èŠ‚å…³é”®è¯ (+2)")
        
        # åŒ…å«åºå·
        if re.search(r'\d+', text):
            medium_indicators += 1
            print(f"    ğŸ”¢ åŒ…å«æ•°å­— (+1)")
        
        # ä¸ä»¥å¥å·ç»“å°¾ï¼ˆæ­£æ–‡é€šå¸¸ä»¥å¥å·ç»“å°¾ï¼‰
        if not text.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?')):
            medium_indicators += 1
            print(f"    ğŸ“„ ä¸ä»¥å¥å·ç»“å°¾ (+1)")
        
        # é•¿åº¦é€‚ä¸­
        if 5 <= len(text) <= 50:
            medium_indicators += 1
            print(f"    ğŸ“ é•¿åº¦é€‚ä¸­ (+1)")
        
        # ä¸åŒ…å«è¿‡å¤šæ ‡ç‚¹ç¬¦å·ï¼ˆæ­£æ–‡é€šå¸¸æ ‡ç‚¹è¾ƒå¤šï¼‰
        punctuation_ratio = len(re.findall(r'[ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹]', text)) / len(text)
        if punctuation_ratio < 0.3:
            medium_indicators += 1
            print(f"    ğŸ“ æ ‡ç‚¹ç¬¦å·å°‘ (+1)")
        
        # åŒ…å«å¸¸è§æ ‡é¢˜è¯æ±‡
        title_words = ['åˆ', 'å§‹', 'æœ«', 'ç»ˆ', 'æ–°', 'è€', 'å¤§', 'å°', 'ä¸Š', 'ä¸‹', 'å‰', 'å', 
                      'ä¸œ', 'è¥¿', 'å—', 'åŒ—', 'å…¥', 'å‡º', 'æ¥', 'å»', 'è§', 'é‡', 'æˆ˜', 'æ–—']
        if any(word in text for word in title_words):
            medium_indicators += 1
            print(f"    ğŸ“š åŒ…å«æ ‡é¢˜å¸¸ç”¨è¯ (+1)")
        
        print(f"    ğŸ“Š æ€»åˆ†: {medium_indicators}/7")
        
        # å¦‚æœç´¯ç§¯æŒ‡æ ‡è¶³å¤Ÿé«˜ï¼Œè®¤ä¸ºæ˜¯æ ‡é¢˜
        if medium_indicators >= 3:
            print(f"    âœ… æ ¹æ®ç»¼åˆæŒ‡æ ‡åˆ¤æ–­ä¸ºæ ‡é¢˜")
            return True
        
        print(f"    âŒ ç»¼åˆæŒ‡æ ‡ä¸è¶³ï¼Œåˆ¤æ–­ä¸ºéæ ‡é¢˜")
        return False
    
    def combine_titles(self, catalog_title, content_title):
        """åˆå¹¶ç›®å½•æ ‡é¢˜å’Œå†…å®¹æ ‡é¢˜"""
        # æ¸…ç†æ ‡é¢˜
        catalog_clean = catalog_title.strip()
        content_clean = content_title.strip()
        
        print(f"    ğŸ”— åˆå¹¶æ ‡é¢˜:")
        print(f"       ç›®å½•: {catalog_clean}")
        print(f"       å†…å®¹: {content_clean}")
        
        # å¦‚æœå†…å®¹æ ‡é¢˜å°±æ˜¯ç›®å½•æ ‡é¢˜çš„ä¸€éƒ¨åˆ†ï¼Œç›´æ¥è¿”å›ç›®å½•æ ‡é¢˜
        if content_clean in catalog_clean:
            print(f"    âœ… å†…å®¹æ ‡é¢˜åŒ…å«åœ¨ç›®å½•æ ‡é¢˜ä¸­ï¼Œä½¿ç”¨ç›®å½•æ ‡é¢˜")
            return catalog_clean
        
        if catalog_clean in content_clean:
            print(f"    âœ… ç›®å½•æ ‡é¢˜åŒ…å«åœ¨å†…å®¹æ ‡é¢˜ä¸­ï¼Œä½¿ç”¨å†…å®¹æ ‡é¢˜")
            return content_clean
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸åŒçš„ç« èŠ‚å·
        catalog_chapter_num = self.extract_chapter_number(catalog_clean)
        content_chapter_num = self.extract_chapter_number(content_clean)
        
        print(f"    ğŸ”¢ ç« èŠ‚å· - ç›®å½•: {catalog_chapter_num}, å†…å®¹: {content_chapter_num}")
        
        if catalog_chapter_num and content_chapter_num and catalog_chapter_num == content_chapter_num:
            # å¦‚æœç« èŠ‚å·ç›¸åŒï¼Œæ™ºèƒ½åˆå¹¶
            catalog_without_num = re.sub(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+[ç« å›èŠ‚å·é›†éƒ¨ç¯‡]\s*', '', catalog_clean)
            content_without_num = re.sub(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+[ç« å›èŠ‚å·é›†éƒ¨ç¯‡]\s*', '', content_clean)
            content_without_num = re.sub(r'^\d+[ç« å›èŠ‚å·é›†éƒ¨ç¯‡]\s*', '', content_without_num)
            
            print(f"    ğŸ“ å»é™¤ç« èŠ‚å·å - ç›®å½•: '{catalog_without_num}', å†…å®¹: '{content_without_num}'")
            
            if catalog_without_num and content_without_num and catalog_without_num != content_without_num:
                result = f"{catalog_clean} {content_without_num}"
                print(f"    âœ… åˆå¹¶ç»“æœ: {result}")
                return result
            else:
                print(f"    âœ… ç« èŠ‚å·ç›¸åŒä¸”å†…å®¹é‡å¤ï¼Œä½¿ç”¨ç›®å½•æ ‡é¢˜")
                return catalog_clean
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„é‡å¤è¯æ±‡
        catalog_words = set(catalog_clean.replace('ç¬¬', '').replace('ç« ', '').replace('å›', '').split())
        content_words = set(content_clean.replace('ç¬¬', '').replace('ç« ', '').replace('å›', '').split())
        
        common_words = catalog_words & content_words
        if len(common_words) > 0 and len(common_words) >= len(catalog_words) * 0.5:
            print(f"    âš ï¸  å‘ç°å¤§é‡é‡å¤è¯æ±‡: {common_words}ï¼Œä½¿ç”¨è¾ƒé•¿çš„æ ‡é¢˜")
            return catalog_clean if len(catalog_clean) >= len(content_clean) else content_clean
        
        # é»˜è®¤åˆå¹¶ï¼šç›®å½•æ ‡é¢˜ + å†…å®¹æ ‡é¢˜
        result = f"{catalog_clean} {content_clean}"
        print(f"    âœ… é»˜è®¤åˆå¹¶ç»“æœ: {result}")
        return result
    
    def extract_chapter_number(self, title):
        """æå–ç« èŠ‚å·"""
        # åŒ¹é…å„ç§ç« èŠ‚å·æ ¼å¼
        patterns = [
            r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡]+)[ç« å›èŠ‚å·é›†éƒ¨ç¯‡]',
            r'ç¬¬(\d+)[ç« å›èŠ‚å·é›†éƒ¨ç¯‡]',
            r'^(\d+)[ç« å›èŠ‚å·é›†éƒ¨ç¯‡]',
            r'[ç¬¬]?(\d+)[ç« å›èŠ‚å·é›†éƒ¨ç¯‡]',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, title)
            if match:
                return match.group(1)
        
        return None
    
    def sort_chapters(self, chapters):
        """å¯¹ç« èŠ‚è¿›è¡Œæ’åºï¼Œç¡®ä¿æ­£ç¡®çš„é˜…è¯»é¡ºåº"""
        print("ğŸ”„ æ­£åœ¨å¯¹ç« èŠ‚è¿›è¡Œæ’åº...")
        
        def extract_chapter_sort_key(chapter_title):
            """æå–ç« èŠ‚çš„æ’åºå…³é”®å­—"""
            title = chapter_title.lower().strip()
            
            # å°è¯•æå–å„ç§æ ¼å¼çš„ç« èŠ‚å·
            patterns = [
                # ä¸­æ–‡æ•°å­—
                (r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡]+)[ç« å›èŠ‚å·é›†éƒ¨ç¯‡]', chinese_to_number),
                # é˜¿æ‹‰ä¼¯æ•°å­—
                (r'ç¬¬(\d+)[ç« å›èŠ‚å·é›†éƒ¨ç¯‡]', int),
                (r'^(\d+)[ç« å›èŠ‚å·é›†éƒ¨ç¯‡]', int),
                (r'[ç« å›èŠ‚å·é›†éƒ¨ç¯‡](\d+)', int),
                # Chapteræ ¼å¼
                (r'chapter\s*(\d+)', int),
                # çº¯æ•°å­—å¼€å¤´
                (r'^(\d+)', int),
            ]
            
            for pattern, converter in patterns:
                match = re.search(pattern, title)
                if match:
                    try:
                        num = converter(match.group(1))
                        return (0, num)  # 0è¡¨ç¤ºæ˜¯æ­£å¸¸ç« èŠ‚ï¼Œnumæ˜¯ç« èŠ‚å·
                    except:
                        continue
            
            # ç‰¹æ®Šå¤„ç†ä¸€äº›å…³é”®è¯
            special_keywords = {
                'åºç« ': (-2, 0),
                'åºè¨€': (-2, 0), 
                'åº': (-2, 0),
                'å‰è¨€': (-2, 0),
                'æ¥”å­': (-1, 0),
                'å¼•å­': (-1, 0),
                'å¼€ç¯‡': (-1, 0),
                'ç»ˆç« ': (999, 999),
                'å°¾å£°': (999, 999),
                'åè®°': (999, 999),
                'ç•ªå¤–': (1000, 0),
            }
            
            for keyword, sort_key in special_keywords.items():
                if keyword in title:
                    return sort_key
            
            # æ— æ³•è¯†åˆ«ç« èŠ‚å·çš„ï¼Œæ”¾åœ¨æœ€å
            return (500, 999999)
        
        # ä¸­æ–‡æ•°å­—è½¬æ¢å‡½æ•°
        def chinese_to_number(chinese_str):
            """å°†ä¸­æ–‡æ•°å­—è½¬æ¢ä¸ºé˜¿æ‹‰ä¼¯æ•°å­—"""
            chinese_dict = {
                'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4, 'äº”': 5,
                'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9, 'å': 10,
                'åä¸€': 11, 'åäºŒ': 12, 'åä¸‰': 13, 'åå››': 14, 'åäº”': 15,
                'åå…­': 16, 'åä¸ƒ': 17, 'åå…«': 18, 'åä¹': 19, 'äºŒå': 20,
                'äºŒåä¸€': 21, 'äºŒåäºŒ': 22, 'äºŒåä¸‰': 23, 'äºŒåå››': 24, 'äºŒåäº”': 25,
                'äºŒåå…­': 26, 'äºŒåä¸ƒ': 27, 'äºŒåå…«': 28, 'äºŒåä¹': 29, 'ä¸‰å': 30,
                'ä¸‰åä¸€': 31, 'ä¸‰åäºŒ': 32, 'ä¸‰åä¸‰': 33, 'ä¸‰åå››': 34, 'ä¸‰åäº”': 35,
                'ä¸‰åå…­': 36, 'ä¸‰åä¸ƒ': 37, 'ä¸‰åå…«': 38, 'ä¸‰åä¹': 39, 'å››å': 40,
                'å››åä¸€': 41, 'å››åäºŒ': 42, 'å››åä¸‰': 43, 'å››åå››': 44, 'å››åäº”': 45,
                'å››åå…­': 46, 'å››åä¸ƒ': 47, 'å››åå…«': 48, 'å››åä¹': 49, 'äº”å': 50,
                'äº”åä¸€': 51, 'äº”åäºŒ': 52, 'äº”åä¸‰': 53, 'äº”åå››': 54, 'äº”åäº”': 55,
                'äº”åå…­': 56, 'äº”åä¸ƒ': 57, 'äº”åå…«': 58, 'äº”åä¹': 59, 'å…­å': 60,
                'å…­åä¸€': 61, 'å…­åäºŒ': 62, 'å…­åä¸‰': 63, 'å…­åå››': 64, 'å…­åäº”': 65,
                'å…­åå…­': 66, 'å…­åä¸ƒ': 67, 'å…­åå…«': 68, 'å…­åä¹': 69, 'ä¸ƒå': 70,
                'ä¸ƒåä¸€': 71, 'ä¸ƒåäºŒ': 72, 'ä¸ƒåä¸‰': 73, 'ä¸ƒåå››': 74, 'ä¸ƒåäº”': 75,
                'ä¸ƒåå…­': 76, 'ä¸ƒåä¸ƒ': 77, 'ä¸ƒåå…«': 78, 'ä¸ƒåä¹': 79, 'å…«å': 80,
                'å…«åä¸€': 81, 'å…«åäºŒ': 82, 'å…«åä¸‰': 83, 'å…«åå››': 84, 'å…«åäº”': 85,
                'å…«åå…­': 86, 'å…«åä¸ƒ': 87, 'å…«åå…«': 88, 'å…«åä¹': 89, 'ä¹å': 90,
                'ä¹åä¸€': 91, 'ä¹åäºŒ': 92, 'ä¹åä¸‰': 93, 'ä¹åå››': 94, 'ä¹åäº”': 95,
                'ä¹åå…­': 96, 'ä¹åä¸ƒ': 97, 'ä¹åå…«': 98, 'ä¹åä¹': 99, 'ä¸€ç™¾': 100,
            }
            
            # å¤„ç†æ›´å¤æ‚çš„ä¸­æ–‡æ•°å­—
            if chinese_str in chinese_dict:
                return chinese_dict[chinese_str]
            
            # å¤„ç†ç™¾ä½æ•°
            if 'ç™¾' in chinese_str:
                parts = chinese_str.split('ç™¾')
                if len(parts) == 2:
                    hundred_part = parts[0] if parts[0] else 'ä¸€'
                    remainder_part = parts[1]
                    
                    hundred_num = chinese_dict.get(hundred_part, 1) * 100
                    remainder_num = chinese_dict.get(remainder_part, 0) if remainder_part else 0
                    
                    return hundred_num + remainder_num
            
            return 0
        
        # ç»™æ¯ä¸ªç« èŠ‚æ·»åŠ æ’åºkey
        chapters_with_key = []
        for chapter in chapters:
            sort_key = extract_chapter_sort_key(chapter['title'])
            chapters_with_key.append((sort_key, chapter))
            print(f"   ğŸ“‹ {chapter['title']} â†’ æ’åºé”®: {sort_key}")
        
        # æŒ‰æ’åºkeyæ’åº
        chapters_with_key.sort(key=lambda x: x[0])
        
        # è¿”å›æ’åºåçš„ç« èŠ‚åˆ—è¡¨
        sorted_chapters = [chapter for _, chapter in chapters_with_key]
        
        print(f"âœ… æ’åºå®Œæˆï¼Œé¡ºåº:")
        for i, chapter in enumerate(sorted_chapters[:10], 1):
            print(f"   {i}. {chapter['title']}")
        if len(sorted_chapters) > 10:
            print(f"   ... è¿˜æœ‰ {len(sorted_chapters) - 10} ä¸ªç« èŠ‚")
        
        return sorted_chapters
    
    def crawl_book(self, delay=3, test_mode=False):
        """çˆ¬å–æ•´æœ¬ä¹¦"""
        if not self.catalog_url:
            print("âŒ æœªè®¾ç½®ç›®å½•URLï¼Œæ— æ³•çˆ¬å–")
            return
        
        mode_text = "æµ‹è¯•æ¨¡å¼ï¼ˆå‰3ç« ï¼‰" if test_mode else "å®Œæ•´æ¨¡å¼ï¼ˆæ‰€æœ‰ç« èŠ‚ï¼‰"
        print(f"ğŸš€ å¼€å§‹çˆ¬å–ã€Š{self.book_info['title']}ã€‹ - {mode_text}")
        print("=" * 60)
        
        # è§£æç« èŠ‚åˆ—è¡¨
        chapters = self.parse_chapter_list()
        
        if not chapters:
            print("âŒ æ— æ³•è·å–ä»»ä½•ç« èŠ‚ä¿¡æ¯")
            return
        
        # æµ‹è¯•æ¨¡å¼ï¼šåªçˆ¬å–å‰å‡ ç« 
        if test_mode:
            chapters = chapters[:3]
            print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šåªçˆ¬å–å‰ {len(chapters)} ç« ")
        else:
            print(f"ğŸš€ å®Œæ•´æ¨¡å¼ï¼šå‡†å¤‡çˆ¬å–æ‰€æœ‰ {len(chapters)} ä¸ªç« èŠ‚")
        
        print(f"ğŸ“š æ‰¾åˆ° {len(chapters)} ä¸ªç« èŠ‚ï¼Œå¼€å§‹çˆ¬å–...")
        print("-" * 60)
        
        # çˆ¬å–ç« èŠ‚å†…å®¹
        chapters_data = []
        success_count = 0
        
        for i, chapter in enumerate(chapters, 1):
            print(f"[{i:3d}/{len(chapters)}] {chapter['title']}")
            
            result = self.get_chapter_content(chapter['url'], chapter['title'])
            merged_title = result["title"]
            content = result["content"]
            
            # è®¡ç®—æ®µè½æ•°é‡
            paragraph_count = content.count('\n\n') + 1 if content else 0
            
            chapter_data = {
                'original_title': chapter['title'],  # ä¿å­˜åŸå§‹ç›®å½•æ ‡é¢˜
                'title': merged_title,               # ä½¿ç”¨åˆå¹¶åçš„æ ‡é¢˜
                'url': chapter['url'],
                'content': content,
                'char_count': len(content),
                'paragraph_count': paragraph_count,
                'success': len(content) > 50 and paragraph_count >= 1
            }
            
            chapters_data.append(chapter_data)
            
            if chapter_data['success']:
                success_count += 1
                if merged_title != chapter['title']:
                    print(f"           âœ… æˆåŠŸ ({paragraph_count} ä¸ªæ®µè½) - æ ‡é¢˜å·²åˆå¹¶")
                else:
                    print(f"           âœ… æˆåŠŸ ({paragraph_count} ä¸ªæ®µè½)")
            else:
                print(f"           âŒ å¤±è´¥æˆ–å†…å®¹ä¸å®Œæ•´")
            
            # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
            if i < len(chapters):  # æœ€åä¸€ç« ä¸éœ€è¦å»¶è¿Ÿ
                print(f"           â³ ç­‰å¾… {delay} ç§’...")
                time.sleep(delay)
        
        # ä¿å­˜ç»“æœ
        print("-" * 60)
        print(f"ğŸ“‹ çˆ¬å–å®Œæˆ:")
        print(f"   æ€»ç« èŠ‚: {len(chapters_data)}")
        print(f"   æˆåŠŸ: {success_count}")
        print(f"   å¤±è´¥: {len(chapters_data) - success_count}")
        
        # ç»Ÿè®¡æ®µè½ä¿¡æ¯
        total_paragraphs = sum(ch.get('paragraph_count', 0) for ch in chapters_data)
        total_chars = sum(ch.get('char_count', 0) for ch in chapters_data)
        print(f"   æ€»æ®µè½æ•°: {total_paragraphs}")
        print(f"   æ€»å­—ç¬¦æ•°: {total_chars:,}")
        
        if success_count > 0:
            # ğŸ¯ åˆ›å»ºä»¥ä¹¦åå‘½åçš„æ–‡ä»¶å¤¹
            safe_title = re.sub(r'[<>:"/\\|?*]', '_', self.book_info['title'])
            safe_title = safe_title.replace('  ', ' ').strip('_').strip()
            
            # ç¡®ä¿æ–‡ä»¶å¤¹åä¸ä¸ºç©º
            if not safe_title or safe_title in ['_', '.', '..']:
                safe_title = f"å°è¯´_{int(time.time())}"
            
            # åˆ›å»ºæ–‡ä»¶å¤¹
            try:
                os.makedirs(safe_title, exist_ok=True)
                print(f"ğŸ“ åˆ›å»ºæ–‡ä»¶å¤¹: {safe_title}")
            except Exception as e:
                print(f"âš ï¸  åˆ›å»ºæ–‡ä»¶å¤¹å¤±è´¥: {e}")
                safe_title = "."  # ä¿å­˜åˆ°å½“å‰ç›®å½•
            
            # ç”Ÿæˆæ–‡ä»¶è·¯å¾„
            txt_path = os.path.join(safe_title, f"{safe_title}.txt")
            json_path = os.path.join(safe_title, f"{safe_title}.json")
            docx_path = os.path.join(safe_title, f"{safe_title}.docx")
            
            # ä¿å­˜æ‰€æœ‰æ ¼å¼
            self.save_to_file(chapters_data, txt_path)
            self.save_to_json(chapters_data, json_path)
            
            # ç”ŸæˆWordæ–‡æ¡£
            if DOCX_AVAILABLE:
                self.save_to_word(chapters_data, docx_path)
            
            print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼è·å¾— {success_count} ä¸ªæœ‰æ•ˆç« èŠ‚ï¼Œå…± {total_paragraphs} ä¸ªæ®µè½")
            print(f"ğŸ“ æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åˆ°æ–‡ä»¶å¤¹: {safe_title}")
            
            return safe_title  # è¿”å›æ–‡ä»¶å¤¹å
        else:
            print("âŒ æ²¡æœ‰æˆåŠŸè·å–ä»»ä½•ç« èŠ‚å†…å®¹")
            print("\nğŸ’¡ å»ºè®®:")
            print("   1. æ£€æŸ¥ç›®å½•URLæ˜¯å¦æ­£ç¡®")
            print("   2. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            print("   3. ç¨åé‡è¯•ï¼ˆå¯èƒ½é‡åˆ°é¢‘ç‡é™åˆ¶ï¼‰")
            print("   4. å°è¯•ä½¿ç”¨VPNæˆ–æ›´æ¢IP")
    
    def save_to_file(self, chapters_data, filename):
        """ä¿å­˜å†…å®¹åˆ°æ–‡ä»¶ - ä¿æŒæ®µè½æ ¼å¼"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(f"{self.book_info['title']}\n")
                f.write("=" * 60 + "\n")
                f.write(f"ä½œè€…ï¼š{self.book_info['author']}\n")
                f.write("çˆ¬å–æ—¶é—´ï¼š" + time.strftime('%Y-%m-%d %H:%M:%S') + "\n")
                f.write(f"æ¥æºï¼š{self.catalog_url}\n")
                f.write("è¯´æ˜ï¼šä¸¥æ ¼æŒ‰ç…§åŸç½‘ç«™pæ ‡ç­¾åˆ†æ®µä¿å­˜ï¼Œæ™ºèƒ½åˆå¹¶æ ‡é¢˜\n")
                f.write("=" * 60 + "\n\n")
                
                total_chars = 0
                total_paragraphs = 0
                
                for i, chapter in enumerate(chapters_data, 1):
                    f.write(f"{chapter['title']}\n")
                    f.write("-" * 50 + "\n\n")
                    
                    if chapter['content']:
                        f.write(chapter['content'])  # å†…å®¹å·²ç»æŒ‰æ®µè½æ ¼å¼åŒ–ï¼Œç›´æ¥å†™å…¥
                        total_chars += len(chapter['content'])
                        total_paragraphs += chapter.get('paragraph_count', 0)
                    else:
                        f.write("[æ­¤ç« èŠ‚å†…å®¹è·å–å¤±è´¥]")
                    
                    f.write(f"\n\n\n")
                
                f.write(f"\n\næ€»è®¡: {len(chapters_data)} ç« , {total_chars:,} å­—, {total_paragraphs} æ®µè½\n")
            
            print(f"âœ… TXTæ–‡ä»¶å·²ä¿å­˜: {filename}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜TXTæ–‡ä»¶å¤±è´¥: {e}")
    
    def save_to_json(self, chapters_data, filename):
        """ä¿å­˜ä¸ºJSONæ ¼å¼"""
        try:
            data = {
                'title': self.book_info['title'],
                'author': self.book_info['author'],
                'crawl_time': time.strftime('%Y-%m-%d %H:%M:%S'),
                'source_url': self.catalog_url,
                'format_note': 'ä¸¥æ ¼æŒ‰ç…§åŸç½‘ç«™pæ ‡ç­¾åˆ†æ®µï¼Œæ™ºèƒ½åˆå¹¶æ ‡é¢˜',
                'total_chapters': len(chapters_data),
                'success_chapters': len([ch for ch in chapters_data if ch.get('success', False)]),
                'total_chars': sum(ch['char_count'] for ch in chapters_data),
                'total_paragraphs': sum(ch.get('paragraph_count', 0) for ch in chapters_data),
                'chapters': chapters_data
            }
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… JSONæ–‡ä»¶å·²ä¿å­˜: {filename}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜JSONå¤±è´¥: {e}")
    
    def save_to_word(self, chapters_data, filename):
        """ä¿å­˜ä¸ºWordæ–‡æ¡£ - ä»¿ç…§ç”¨æˆ·æä¾›çš„æ’ç‰ˆæ ¼å¼"""
        try:
            print("ğŸ“ æ­£åœ¨ç”ŸæˆWordæ–‡æ¡£...")
            
            # åˆ›å»ºæ–°çš„Wordæ–‡æ¡£
            document = Document()
            
            # è®¾ç½®ä¸­æ–‡å­—ä½“ - å…¨å±€æ ·å¼
            document.styles['Normal'].font.name = 'æ¥·ä½“'
            document.styles['Normal']._element.rPr.rFonts.set(qn('w:eastAsia'), 'æ¥·ä½“')
            document.styles['Normal'].font.size = Pt(14)  # å››å·å­—
            
            # ğŸ“ æ–‡æ¡£æ ‡é¢˜ - æ¥·ä½“
            title = document.add_heading(self.book_info['title'], 0)
            title.alignment = WD_ALIGN_PARAGRAPH.CENTER
            title.runs[0].font.name = 'æ¥·ä½“'
            title.runs[0]._element.rPr.rFonts.set(qn('w:eastAsia'), 'æ¥·ä½“')
            title.runs[0].font.size = Pt(18)  # å°äºŒå·
            title.runs[0].bold = True
            
            # ç©ºè¡Œ
            document.add_paragraph()
            
            # ğŸ“ ä½œè€…ä¿¡æ¯ - æ¥·ä½“
            author_para = document.add_paragraph()
            author_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
            author_run = author_para.add_run(f'ä½œè€…ï¼š{self.book_info["author"]}')
            author_run.font.name = 'æ¥·ä½“'
            author_run._element.rPr.rFonts.set(qn('w:eastAsia'), 'æ¥·ä½“')
            author_run.font.size = Pt(14)  # å››å·å­—
            
            # ğŸ¯ ä½œè€…ä¿¡æ¯åæ’å…¥åˆ†é¡µç¬¦
            document.add_page_break()
            
            # ğŸ“ æ·»åŠ ç« èŠ‚å†…å®¹ - ä»¿ç…§ç”¨æˆ·æ’ç‰ˆ
            success_count = 0
            
            for i, chapter in enumerate(chapters_data, 1):
                if not chapter.get('success', False) or not chapter.get('content'):
                    continue
                    
                success_count += 1
                print(f"ğŸ“„ æ­£åœ¨å¤„ç†ç¬¬ {success_count} ç« : {chapter['title']}")
                
                # ğŸ¯ ç« å›æ ‡é¢˜ - ä½¿ç”¨æ ‡é¢˜1æ ·å¼ï¼Œæ¥·ä½“å°äºŒå·
                title_heading = document.add_heading(chapter['title'], 1)  # æ ‡é¢˜1æ ·å¼
                title_heading.alignment = WD_ALIGN_PARAGRAPH.CENTER        # å±…ä¸­å¯¹é½
                title_heading.runs[0].font.name = 'æ¥·ä½“'
                title_heading.runs[0]._element.rPr.rFonts.set(qn('w:eastAsia'), 'æ¥·ä½“')
                title_heading.runs[0].font.size = Pt(18)                   # å°äºŒå· = 18pt
                title_heading.runs[0].bold = True
                
                # è°ƒæ•´æ ‡é¢˜åé—´è·
                title_heading.paragraph_format.space_after = Pt(6)         # å‡å°‘æ ‡é¢˜åé—´è·
                
                # ğŸ“ ç« èŠ‚å†…å®¹ - ä»¿ç…§ç”¨æˆ·çš„æ®µè½æ ¼å¼
                content = chapter['content']
                paragraphs = content.split('\n\n')  # æŒ‰åŒæ¢è¡Œåˆ†æ®µ
                
                for para_text in paragraphs:
                    para_text = para_text.strip()
                    if para_text:
                        # ğŸ¯ åˆ›å»ºæ®µè½ - é¦–è¡Œç¼©è¿›ä¸¤ä¸ªå­—ç¬¦
                        para = document.add_paragraph()
                        para_run = para.add_run(para_text)
                        para_run.font.name = 'æ¥·ä½“'
                        para_run._element.rPr.rFonts.set(qn('w:eastAsia'), 'æ¥·ä½“')
                        para_run.font.size = Pt(12)
                        
                        # ğŸ¯ æ®µè½æ ¼å¼ - é¦–è¡Œç¼©è¿›ä¸¤ä¸ªå­—ç¬¦
                        para.paragraph_format.first_line_indent = Pt(24)      # é¦–è¡Œç¼©è¿›ä¸¤ä¸ªå­—ç¬¦ (12pt * 2 = 24pt)
                        para.paragraph_format.left_indent = Inches(0)         # å·¦å¯¹é½
                        para.paragraph_format.space_after = Pt(8)             # æ®µåé—´è·
                        para.paragraph_format.line_spacing = 1.15             # è¡Œè·
                        para.alignment = WD_ALIGN_PARAGRAPH.LEFT              # å·¦å¯¹é½
                
                # ç« èŠ‚ç»“æŸï¼ˆä¸æ·»åŠ é¢å¤–ç©ºè¡Œï¼Œç›´æ¥åˆ†é¡µï¼‰
                
                # ğŸ¯ æ¯ä¸€å›ç»“æŸåæ·»åŠ åˆ†é¡µç¬¦ï¼ˆé™¤äº†æœ€åä¸€å›ï¼‰
                if success_count < len([ch for ch in chapters_data if ch.get('success', False)]):
                    document.add_page_break()
            
            # ä¿å­˜Wordæ–‡æ¡£
            document.save(filename)
            print(f"âœ… Wordæ–‡æ¡£å·²ä¿å­˜: {filename}")
            print(f"ğŸ“Š æˆåŠŸå¤„ç† {success_count} ä¸ªç« èŠ‚")
            print(f"ğŸ¨ æ’ç‰ˆç‰¹ç‚¹ï¼šæ¥·ä½“å­—ä½“ã€æ ‡é¢˜1æ ·å¼å°äºŒå·ã€æ­£æ–‡å››å·å­—ã€ç´§å‡‘é—´è·ã€æ¯å›åˆ†é¡µ")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜Wordæ–‡æ¡£å¤±è´¥: {e}")
            print("ğŸ’¡ å¯èƒ½æ˜¯python-docxåº“é—®é¢˜ï¼Œè¯·æ£€æŸ¥å®‰è£…ï¼špip install python-docx")

def main():
    """ä¸»å‡½æ•° - åŒ…å«å®Œæ•´çš„ç”¨æˆ·é€‰æ‹©ç•Œé¢"""
    print("=" * 60)
    print("        é€šç”¨å°è¯´çˆ¬è™« - åŠ¨æ€URLç‰ˆæœ¬ v3.0")
    print("        (æ”¯æŒä»»æ„å°è¯´ç½‘ç«™ç›®å½•é¡µé¢)")
    print("=" * 60)
    print("ğŸ¯ æ”¯æŒå¤§éƒ¨åˆ†å°è¯´ç½‘ç«™çš„ç›®å½•é¡µé¢")
    print("ğŸ“ è‡ªåŠ¨è¯†åˆ«ä¹¦åã€ä½œè€…å’Œç« èŠ‚åˆ—è¡¨")
    print("ğŸ”— æ™ºèƒ½åˆå¹¶ç›®å½•æ ‡é¢˜å’Œå†…å®¹æ ‡é¢˜")
    print("ğŸ“„ çˆ¬å–å®Œæˆåè‡ªåŠ¨ç”Ÿæˆæ ¼å¼åŒ–Wordæ–‡æ¡£")
    print("âš ï¸  è¯·éµå®ˆç½‘ç«™ä½¿ç”¨æ¡æ¬¾ï¼Œä»…ç”¨äºå­¦ä¹ ç ”ç©¶")
    print("=" * 60)
    
    # è·å–ç”¨æˆ·è¾“å…¥çš„ç›®å½•URL
    while True:
        print("\nè¯·è¾“å…¥å°è¯´ç›®å½•é¡µé¢çš„URL:")
        print("ğŸ’¡ ç¤ºä¾‹:")
        print("   https://www.gushiwen.cn/guwen/book_ce3ab505d8e6.aspx")
        print("   https://www.æŸå°è¯´ç½‘ç«™.com/book/12345/")
        print("   https://m.æŸç½‘ç«™.com/novel/ç›®å½•é¡µé¢")
        print()
        
        catalog_url = input("ğŸ“ ç›®å½•URL: ").strip()
        
        if not catalog_url:
            print("âŒ URLä¸èƒ½ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥")
            continue
        
        # ç®€å•éªŒè¯URLæ ¼å¼
        if not catalog_url.startswith(('http://', 'https://')):
            print("âŒ è¯·è¾“å…¥å®Œæ•´çš„URLï¼ˆåŒ…å«http://æˆ–https://ï¼‰")
            continue
        
        # æµ‹è¯•URLæ˜¯å¦å¯è®¿é—®
        print(f"ğŸ” æ­£åœ¨æµ‹è¯•URLè®¿é—®æ€§: {catalog_url}")
        
        try:
            test_headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            test_response = requests.get(catalog_url, headers=test_headers, timeout=10)
            
            if test_response.status_code == 200:
                print("âœ… URLå¯ä»¥æ­£å¸¸è®¿é—®")
                break
            else:
                print(f"âš ï¸  URLè¿”å›çŠ¶æ€ç  {test_response.status_code}ï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ")
                continue_choice = input("ç»§ç»­ä½¿ç”¨æ­¤URLå—ï¼Ÿ(y/n): ").strip().lower()
                if continue_choice in ['y', 'yes', 'æ˜¯']:
                    break
                else:
                    continue
        except Exception as e:
            print(f"âš ï¸  URLè®¿é—®æµ‹è¯•å¤±è´¥: {e}")
            continue_choice = input("ä»è¦ä½¿ç”¨æ­¤URLå—ï¼Ÿ(y/n): ").strip().lower()
            if continue_choice in ['y', 'yes', 'æ˜¯']:
                break
            else:
                continue
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = UniversalNovelCrawler(catalog_url)
    
    # è¯¢é—®ç”¨æˆ·æƒ³è¦çš„æ¨¡å¼
    while True:
        print("\nè¯·é€‰æ‹©çˆ¬å–æ¨¡å¼:")
        print("1. ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼ˆåªçˆ¬å–å‰3ç« ï¼Œå¿«é€ŸéªŒè¯æ•ˆæœï¼‰")
        print("2. ğŸš€ å®Œæ•´æ¨¡å¼ï¼ˆçˆ¬å–æ‰€æœ‰ç« èŠ‚ï¼Œç”Ÿæˆå®Œæ•´Wordæ–‡æ¡£ï¼‰")
        print("3. ğŸ“Š å…ˆæµ‹è¯•å†å†³å®šï¼ˆæ¨èï¼‰")
        print("4. âŒ é€€å‡ºç¨‹åº")
        
        try:
            choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1/2/3/4): ").strip()
            
            if choice == "1":
                print("\n" + "="*50)
                print("ğŸ§ª å·²é€‰æ‹©ï¼šæµ‹è¯•æ¨¡å¼")
                print("ğŸ“ å°†çˆ¬å–å‰3ç« å¹¶ç”Ÿæˆæµ‹è¯•Wordæ–‡æ¡£...")
                crawler.crawl_book(delay=2, test_mode=True)
                break
                
            elif choice == "2":
                print("\n" + "="*50)
                print("ğŸš€ å·²é€‰æ‹©ï¼šå®Œæ•´æ¨¡å¼")
                print("ğŸ“š å°†çˆ¬å–æ‰€æœ‰ç« èŠ‚å¹¶ç”Ÿæˆå®Œæ•´Wordæ–‡æ¡£...")
                
                # äºŒæ¬¡ç¡®è®¤
                confirm = input("âš ï¸  å®Œæ•´çˆ¬å–å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œç¡®è®¤ç»§ç»­ï¼Ÿ(y/n): ").strip().lower()
                if confirm in ['y', 'yes', 'æ˜¯', 'ç¡®è®¤']:
                    crawler.crawl_book(delay=3, test_mode=False)
                    break
                else:
                    print("âŒ å·²å–æ¶ˆå®Œæ•´çˆ¬å–ï¼Œè¿”å›ä¸»èœå•")
                    continue
                
            elif choice == "3":
                print("\n" + "="*50)
                print("ğŸ“Š æ¨èæ¨¡å¼ï¼šå…ˆæµ‹è¯•å†å†³å®š")
                print("ğŸ§ª é¦–å…ˆè¿›è¡Œæµ‹è¯•æ¨¡å¼ï¼ˆå‰3ç« ï¼‰...")
                test_folder = crawler.crawl_book(delay=2, test_mode=True)
                
                print("\n" + "="*40)
                print("ğŸ“‹ æµ‹è¯•é˜¶æ®µå®Œæˆï¼")
                if test_folder:
                    print(f"ğŸ“ æµ‹è¯•æ–‡ä»¶å·²ä¿å­˜åˆ°æ–‡ä»¶å¤¹: {test_folder}")
                
                continue_choice = input("âœ¨ æ•ˆæœæ»¡æ„å—ï¼Ÿæ˜¯å¦ç»§ç»­çˆ¬å–å®Œæ•´ç‰ˆæœ¬ï¼Ÿ(y/n): ").strip().lower()
                
                if continue_choice in ['y', 'yes', 'æ˜¯', 'æ»¡æ„']:
                    print("\nğŸš€ å¼€å§‹å®Œæ•´çˆ¬å–å¹¶ç”Ÿæˆå®Œæ•´Wordæ–‡æ¡£...")
                    final_folder = crawler.crawl_book(delay=3, test_mode=False)
                    if final_folder:
                        print(f"ğŸ“ å®Œæ•´ç‰ˆæ–‡ä»¶å·²ä¿å­˜åˆ°æ–‡ä»¶å¤¹: {final_folder}")
                else:
                    print("ğŸ‘‹ æµ‹è¯•å®Œæˆï¼Œæ„Ÿè°¢ä½¿ç”¨ï¼")
                break
                
            elif choice == "4":
                print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨é€šç”¨å°è¯´çˆ¬è™«ï¼å†è§ï¼")
                break
                
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1ã€2ã€3 æˆ– 4")
                
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­ï¼Œå†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ è¾“å…¥å¤„ç†é”™è¯¯: {e}")
            print("è¯·é‡æ–°é€‰æ‹©...")
            continue
    
    print(f"\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"   ğŸ“„ [ä¹¦å].txt - çº¯æ–‡æœ¬æ ¼å¼")
    print(f"   ğŸ“‹ [ä¹¦å].json - æ•°æ®æ ¼å¼ï¼ˆåŒ…å«å…ƒæ•°æ®ï¼‰")
    if DOCX_AVAILABLE:
        print(f"   ğŸ“ [ä¹¦å].docx - Wordæ–‡æ¡£ï¼ˆæ ‡é¢˜åŠ ç²—ï¼Œæ ¼å¼åŒ–ï¼‰")
    else:
        print(f"   âš ï¸  Wordæ–‡æ¡£æœªç”Ÿæˆï¼ˆéœ€è¦å®‰è£…ï¼špip install python-docxï¼‰")
    
    print(f"\nğŸ‰ æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åœ¨ä»¥ä¹¦åå‘½åçš„æ–‡ä»¶å¤¹ä¸­ï¼")
    print(f"ğŸ“ æ–‡ä»¶å¤¹ç»“æ„: [ä¹¦å]/[ä¹¦å].txt, [ä¹¦å].json, [ä¹¦å].docx")
    print(f"ğŸ”„ ç« èŠ‚å·²æŒ‰æ­£ç¡®é¡ºåºæ’åºï¼Œä»ç¬¬ä¸€ç« å¼€å§‹")
    print(f"ğŸ’¡ æç¤ºï¼šæ–‡ä»¶å¤¹å’Œæ–‡ä»¶åæ ¹æ®è‡ªåŠ¨è¯†åˆ«çš„ä¹¦åç”Ÿæˆ")

if __name__ == "__main__":
    main()