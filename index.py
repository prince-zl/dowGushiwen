import requests
from bs4 import BeautifulSoup
import time
import os
import json
import re
from urllib.parse import urljoin

# Wordæ–‡æ¡£ç›¸å…³å¯¼å…¥
try:
    from docx import Document
    from docx.shared import Inches, Pt
    from docx.enum.text import WD_ALIGN_PARAGRAPH
    from docx.oxml.ns import qn
    DOCX_AVAILABLE = True
    print("âœ… python-docx å·²å®‰è£…ï¼Œå°†è‡ªåŠ¨ç”ŸæˆWordæ–‡æ¡£")
except ImportError:
    DOCX_AVAILABLE = False
    print("âš ï¸  python-docx æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install python-docx")
    print("   å°†åªç”ŸæˆTXTå’ŒJSONæ–‡ä»¶")

class JigongCrawler:
    def __init__(self):
        self.base_url = "https://m.gushiwen.cn"  # ä½¿ç”¨ç§»åŠ¨ç‰ˆï¼Œæ›´ç¨³å®š
        self.desktop_url = "https://www.gushiwen.cn"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Connection': 'keep-alive',
            'Referer': 'https://m.gushiwen.cn/',
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
    def generate_known_chapters(self):
        """åŸºäºæœç´¢ç»“æœç”Ÿæˆå·²çŸ¥å­˜åœ¨çš„ç« èŠ‚"""
        chapters = []
        
        # ä»æœç´¢ç»“æœä¸­è·å¾—çš„ç¡®å®å­˜åœ¨çš„ç« èŠ‚
        known_chapters = [
            # åŸºäºæœç´¢ç»“æœçš„çœŸå®URL
            ("ç¬¬ä¸€ç™¾å…­åä¸€å› é€›è¥¿æ¹–æ¶éœ¸é‡å¦–é£ çœ‹åˆè¯­ç§è®¿ç™½é±¼å¯º", 
             "https://m.gushiwen.cn/guwen/bookv_098fbba030de.aspx"),
        ]
        
        # æ·»åŠ å·²çŸ¥ç« èŠ‚
        for title, url in known_chapters:
            chapters.append({'title': title, 'url': url})
        
        print(f"âœ… å‡†å¤‡äº† {len(chapters)} ä¸ªç¡®è®¤å­˜åœ¨çš„ç« èŠ‚")
        return chapters
    
    def try_find_chapter_pattern(self):
        """å°è¯•å‘ç°ç« èŠ‚URLè§„å¾‹"""
        print("ğŸ” åˆ†æç½‘ç«™ç« èŠ‚URLæ¨¡å¼...")
        
        # ç”±äºæˆ‘ä»¬çŸ¥é“ä¸»é¡µé¢æœ‰ç« èŠ‚é“¾æ¥ï¼Œå°è¯•ä¸åŒçš„æ–¹æ³•è·å–
        main_page_urls = [
            "https://www.gushiwen.cn/guwen/book_ce3ab505d8e6.aspx",
            "https://m.gushiwen.cn/guwen/book_ce3ab505d8e6.aspx"
        ]
        
        chapters = []
        
        for url in main_page_urls:
            try:
                print(f"å°è¯•åˆ†æ: {url}")
                response = self.session.get(url, timeout=15)
                
                if response.status_code == 200:
                    # æ£€æŸ¥æ˜¯å¦æœ‰JavaScriptåŠ¨æ€åŠ è½½çš„å†…å®¹
                    if 'bookv_' in response.text:
                        print("âœ… å‘ç°bookv_æ¨¡å¼çš„é“¾æ¥")
                        
                        # ä½¿ç”¨æ­£åˆ™æå–æ‰€æœ‰å¯èƒ½çš„ç« èŠ‚ID
                        pattern = r'bookv_([a-f0-9]{12})\.aspx'
                        matches = re.findall(pattern, response.text)
                        
                        # ä¸é™åˆ¶æ•°é‡ï¼Œå‘ç°æ‰€æœ‰ç« èŠ‚
                        for i, chapter_id in enumerate(matches, 1):
                            chapter_url = f"https://m.gushiwen.cn/guwen/bookv_{chapter_id}.aspx"
                            chapters.append({
                                'title': f'ç¬¬{i}å›',
                                'url': chapter_url
                            })
                            print(f"  å‘ç°ç« èŠ‚: bookv_{chapter_id}.aspx")
                
            except Exception as e:
                print(f"åˆ†æ {url} å¤±è´¥: {e}")
                continue
        
        if chapters:
            print(f"ğŸ¯ é€šè¿‡æ¨¡å¼åˆ†æå‘ç° {len(chapters)} ä¸ªç« èŠ‚")
        else:
            print("âš ï¸  æ— æ³•é€šè¿‡æ¨¡å¼åˆ†æå‘ç°ç« èŠ‚")
            
        return chapters
    
    def get_chapter_content(self, chapter_url, chapter_title):
        """è·å–å•ä¸ªç« èŠ‚å†…å®¹ - ä¸¥æ ¼æŒ‰ç…§pæ ‡ç­¾åˆ†æ®µ"""
        try:
            print(f"  ğŸ“– è·å–: {chapter_title}")
            
            response = self.session.get(chapter_url, timeout=15)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                print(f"    âŒ HTTPé”™è¯¯: {response.status_code}")
                return ""
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                element.decompose()
            
            content = ""
            
            # ç­–ç•¥1: ä¸¥æ ¼æŒ‰pæ ‡ç­¾åˆ†æ®µ - é’ˆå¯¹å¤è¯—æ–‡ç½‘çš„ç‰¹å®šç»“æ„
            gushiwen_selectors = [
                '.main3 .left .cont',  # å¤è¯—æ–‡ç½‘æ¡Œé¢ç‰ˆå¸¸ç”¨ç»“æ„
                '.main3 .cont',        # ç®€åŒ–ç‰ˆ
                '.cont',               # ä¸»å†…å®¹åŒº
                '[class*="main"] .cont',
                '#main .cont',
                '.left .cont',
                '.content',
                '.main-content'
            ]
            
            for selector in gushiwen_selectors:
                content_container = soup.select_one(selector)
                if content_container:
                    print(f"    âœ… æ‰¾åˆ°å†…å®¹å®¹å™¨: {selector}")
                    
                    # ä¸¥æ ¼æå–æ‰€æœ‰pæ ‡ç­¾ä½œä¸ºæ®µè½
                    paragraphs = content_container.find_all('p')
                    
                    if paragraphs:
                        paragraph_texts = []
                        for p in paragraphs:
                            p_text = p.get_text(strip=True)
                            if p_text and len(p_text) > 5:  # è¿‡æ»¤å¤ªçŸ­çš„æ®µè½
                                paragraph_texts.append(p_text)
                        
                        if paragraph_texts:
                            content = '\n\n'.join(paragraph_texts)  # æ¯ä¸ªpæ ‡ç­¾é—´ç”¨åŒæ¢è¡Œåˆ†éš”
                            print(f"    âœ… ä¸¥æ ¼æŒ‰pæ ‡ç­¾æå–åˆ° {len(paragraph_texts)} ä¸ªæ®µè½")
                            break
                    
                    # å¦‚æœå®¹å™¨å†…æ²¡æœ‰pæ ‡ç­¾ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–æ®µè½æ ‡è®°
                    if not content:
                        # æ£€æŸ¥æ˜¯å¦æœ‰divæˆ–spanä½œä¸ºæ®µè½åˆ†éš”
                        div_paragraphs = content_container.find_all(['div', 'span'])
                        if div_paragraphs and len(div_paragraphs) > 1:
                            para_texts = []
                            for div in div_paragraphs:
                                div_text = div.get_text(strip=True)
                                if div_text and len(div_text) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„å†…å®¹
                                    para_texts.append(div_text)
                            
                            if para_texts and len(para_texts) > 1:
                                content = '\n\n'.join(para_texts)
                                print(f"    âœ… æŒ‰div/spanæ ‡ç­¾æå–åˆ° {len(para_texts)} ä¸ªæ®µè½")
                                break
                        
                        # å¦‚æœéƒ½æ²¡æœ‰ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰bræ ‡ç­¾åˆ†å‰²çš„å†…å®¹
                        container_html = str(content_container)
                        if '<br' in container_html.lower():
                            # å°†bræ ‡ç­¾æ›¿æ¢ä¸ºæ®µè½åˆ†éš”ç¬¦
                            br_separated = re.sub(r'<br[^>]*?/?>', '\n||PARAGRAPH_BREAK||\n', container_html)
                            # ç§»é™¤å…¶ä»–HTMLæ ‡ç­¾
                            clean_text = BeautifulSoup(br_separated, 'html.parser').get_text()
                            # æŒ‰æ®µè½åˆ†éš”ç¬¦åˆ†æ®µ
                            paragraphs = [p.strip() for p in clean_text.split('||PARAGRAPH_BREAK||')]
                            paragraphs = [p for p in paragraphs if p and len(p) > 10]
                            
                            if paragraphs:
                                content = '\n\n'.join(paragraphs)
                                print(f"    âœ… æŒ‰bræ ‡ç­¾åˆ†æ®µæå–åˆ° {len(paragraphs)} ä¸ªæ®µè½")
                                break
            
            # ç­–ç•¥2: å¦‚æœå®¹å™¨ç­–ç•¥å¤±è´¥ï¼Œç›´æ¥åœ¨æ•´ä¸ªé¡µé¢ä¸­æŸ¥æ‰¾æ‰€æœ‰pæ ‡ç­¾
            if not content or len(content) < 200:
                print(f"    ğŸ”§ ç­–ç•¥1å¤±è´¥ï¼Œåœ¨æ•´ä¸ªé¡µé¢æŸ¥æ‰¾pæ ‡ç­¾...")
                
                all_paragraphs = soup.find_all('p')
                
                if all_paragraphs:
                    paragraph_texts = []
                    for p in all_paragraphs:
                        p_text = p.get_text(strip=True)
                        # è¿‡æ»¤æ‰æ˜æ˜¾çš„å¯¼èˆªã€èœå•ã€ç‰ˆæƒä¿¡æ¯
                        if (p_text and len(p_text) > 15 and 
                            not any(skip in p_text.lower() for skip in 
                                   ['å¯¼èˆª', 'èœå•', 'ç™»å½•', 'æ³¨å†Œ', 'é¦–é¡µ', 'ç‰ˆæƒ', 'copyright', 
                                    'å…³äºæˆ‘ä»¬', 'è”ç³»æˆ‘ä»¬', 'ç”¨æˆ·åè®®', 'éšç§æ”¿ç­–', 'æ„è§åé¦ˆ'])):
                            paragraph_texts.append(p_text)
                    
                    if paragraph_texts and len(paragraph_texts) > 2:
                        content = '\n\n'.join(paragraph_texts)
                        print(f"    âœ… ä»å…¨é¡µé¢ä¸¥æ ¼æŒ‰pæ ‡ç­¾æå–åˆ° {len(paragraph_texts)} ä¸ªæ®µè½")
            
            # ç­–ç•¥3: æŸ¥æ‰¾å…·æœ‰æ˜ç¡®æ®µè½ç»“æ„çš„å¤§æ–‡æœ¬å—
            if not content or len(content) < 100:
                print(f"    ğŸ”§ ç­–ç•¥2å¤±è´¥ï¼ŒæŸ¥æ‰¾ç»“æ„åŒ–æ–‡æœ¬å—...")
                
                # æŸ¥æ‰¾å¯èƒ½åŒ…å«æ•…äº‹å†…å®¹çš„å…ƒç´ 
                content_elements = soup.find_all(['div', 'article', 'section', 'main'])
                
                best_content = ""
                best_paragraph_count = 0
                
                for elem in content_elements:
                    # æ£€æŸ¥è¯¥å…ƒç´ å†…çš„æ®µè½ç»“æ„
                    elem_paragraphs = elem.find_all('p')
                    
                    if elem_paragraphs and len(elem_paragraphs) >= 3:  # è‡³å°‘3ä¸ªæ®µè½æ‰è€ƒè™‘
                        para_texts = []
                        for p in elem_paragraphs:
                            p_text = p.get_text(strip=True)
                            if p_text and len(p_text) > 10:
                                para_texts.append(p_text)
                        
                        # è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯æ­£æ–‡çš„å†…å®¹
                        filtered_paras = []
                        for para in para_texts:
                            if not any(skip in para.lower() for skip in 
                                     ['å¯¼èˆª', 'èœå•', 'ç™»å½•', 'æ³¨å†Œ', 'é¦–é¡µ', 'javascript:', 
                                      'ç‰ˆæƒæ‰€æœ‰', 'å…³äºæˆ‘ä»¬', 'è”ç³»æˆ‘ä»¬', 'ç”¨æˆ·åè®®']):
                                filtered_paras.append(para)
                        
                        if len(filtered_paras) > best_paragraph_count and len(filtered_paras) >= 3:
                            best_content = '\n\n'.join(filtered_paras)
                            best_paragraph_count = len(filtered_paras)
                
                if best_content:
                    content = best_content
                    print(f"    âœ… æ‰¾åˆ°ç»“æ„åŒ–å†…å®¹å—ï¼Œä¸¥æ ¼æŒ‰æ®µè½åˆ†å‰² ({best_paragraph_count} ä¸ªæ®µè½)")
            
            # æœ€ç»ˆå†…å®¹éªŒè¯å’Œæ ¼å¼åŒ–
            if content:
                # æ¸…ç†å¤šä½™çš„ç©ºè¡Œï¼Œä½†ä¿æŒåŒæ¢è¡Œçš„æ®µè½åˆ†éš”
                content = re.sub(r'\n\s*\n\s*\n+', '\n\n', content)
                content = content.strip()
                
                # éªŒè¯å†…å®¹è´¨é‡
                paragraph_count = content.count('\n\n') + 1
                
                if len(content) > 100 and paragraph_count >= 2:
                    print(f"    âœ… æœ€ç»ˆæˆåŠŸ ({len(content)} å­—ç¬¦, {paragraph_count} ä¸ªæ®µè½)")
                    print(f"    ğŸ“‹ æ®µè½é¢„è§ˆ: {content[:100]}...")
                    return content
                else:
                    print(f"    âš ï¸  å†…å®¹è´¨é‡ä¸è¶³ ({len(content)} å­—ç¬¦, {paragraph_count} ä¸ªæ®µè½)")
            
            print(f"    âŒ æ‰€æœ‰ç­–ç•¥éƒ½æœªèƒ½æå–åˆ°æœ‰æ•ˆçš„åˆ†æ®µå†…å®¹")
            
            # è°ƒè¯•ä¿¡æ¯
            print(f"    ğŸ” è°ƒè¯•ä¿¡æ¯:")
            print(f"       é¡µé¢æ ‡é¢˜: {soup.title.string if soup.title else 'æ— æ ‡é¢˜'}")
            print(f"       é¡µé¢å¤§å°: {len(response.text)} å­—ç¬¦")
            print(f"       pæ ‡ç­¾æ•°é‡: {len(soup.find_all('p'))}")
            print(f"       æ˜¯å¦åŒ…å«'è¯è¯´': {'è¯è¯´' in response.text}")
            print(f"       æ˜¯å¦åŒ…å«'æµå…¬': {'æµå…¬' in response.text}")
            
            return ""
            
        except Exception as e:
            print(f"    âŒ è·å–å¤±è´¥: {e}")
            return ""
    
    def crawl_book(self, delay=3, test_mode=False):
        """çˆ¬å–æ•´æœ¬ä¹¦"""
        mode_text = "æµ‹è¯•æ¨¡å¼ï¼ˆå‰3ç« ï¼‰" if test_mode else "å®Œæ•´æ¨¡å¼ï¼ˆæ‰€æœ‰ç« èŠ‚ï¼‰"
        print(f"ğŸš€ å¼€å§‹çˆ¬å–ã€Šæµå…¬å…¨ä¼ ã€‹ - {mode_text}")
        print("=" * 60)
        
        # è·å–ç« èŠ‚åˆ—è¡¨
        chapters = []
        
        # æ–¹æ³•1: å°è¯•æ¨¡å¼åˆ†æ
        pattern_chapters = self.try_find_chapter_pattern()
        chapters.extend(pattern_chapters)
        
        # æ–¹æ³•2: ä½¿ç”¨å·²çŸ¥ç« èŠ‚
        known_chapters = self.generate_known_chapters()
        chapters.extend(known_chapters)
        
        # å»é™¤é‡å¤
        unique_chapters = []
        seen_urls = set()
        for chapter in chapters:
            if chapter['url'] not in seen_urls:
                unique_chapters.append(chapter)
                seen_urls.add(chapter['url'])
        
        chapters = unique_chapters
        
        if not chapters:
            print("âŒ æ— æ³•è·å–ä»»ä½•ç« èŠ‚ä¿¡æ¯")
            return
        
        # æµ‹è¯•æ¨¡å¼ï¼šåªçˆ¬å–å‰å‡ ç« 
        if test_mode:
            chapters = chapters[:3]
            print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šåªçˆ¬å–å‰ {len(chapters)} ç« ")
        else:
            print(f"ğŸš€ å®Œæ•´æ¨¡å¼ï¼šå‡†å¤‡çˆ¬å–æ‰€æœ‰ {len(chapters)} ä¸ªç« èŠ‚")
        
        print(f"ğŸ“š æ‰¾åˆ° {len(chapters)} ä¸ªç« èŠ‚ï¼Œå¼€å§‹çˆ¬å–...")
        print("-" * 60)
        
        # çˆ¬å–ç« èŠ‚å†…å®¹
        chapters_data = []
        success_count = 0
        
        for i, chapter in enumerate(chapters, 1):
            print(f"[{i:3d}/{len(chapters)}] {chapter['title']}")
            
            content = self.get_chapter_content(chapter['url'], chapter['title'])
            
            # è®¡ç®—æ®µè½æ•°é‡
            paragraph_count = content.count('\n\n') + 1 if content else 0
            
            chapter_data = {
                'title': chapter['title'],
                'url': chapter['url'],
                'content': content,
                'char_count': len(content),
                'paragraph_count': paragraph_count,
                'success': len(content) > 100 and paragraph_count >= 2
            }
            
            chapters_data.append(chapter_data)
            
            if chapter_data['success']:
                success_count += 1
                print(f"           âœ… æˆåŠŸ ({paragraph_count} ä¸ªæ®µè½)")
            else:
                print(f"           âŒ å¤±è´¥æˆ–å†…å®¹ä¸å®Œæ•´")
            
            # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
            if i < len(chapters):  # æœ€åä¸€ç« ä¸éœ€è¦å»¶è¿Ÿ
                print(f"           â³ ç­‰å¾… {delay} ç§’...")
                time.sleep(delay)
        
        # ä¿å­˜ç»“æœ
        print("-" * 60)
        print(f"ğŸ“‹ çˆ¬å–å®Œæˆ:")
        print(f"   æ€»ç« èŠ‚: {len(chapters_data)}")
        print(f"   æˆåŠŸ: {success_count}")
        print(f"   å¤±è´¥: {len(chapters_data) - success_count}")
        
        # ç»Ÿè®¡æ®µè½ä¿¡æ¯
        total_paragraphs = sum(ch.get('paragraph_count', 0) for ch in chapters_data)
        total_chars = sum(ch.get('char_count', 0) for ch in chapters_data)
        print(f"   æ€»æ®µè½æ•°: {total_paragraphs}")
        print(f"   æ€»å­—ç¬¦æ•°: {total_chars:,}")
        
        if success_count > 0:
            # ä¿å­˜æ‰€æœ‰æ ¼å¼
            self.save_to_file(chapters_data)
            self.save_to_json(chapters_data)
            
            # ğŸ¯ é‡ç‚¹ï¼šç›´æ¥ç”ŸæˆWordæ–‡æ¡£
            if DOCX_AVAILABLE:
                self.save_to_word(chapters_data)
            
            print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼è·å¾— {success_count} ä¸ªæœ‰æ•ˆç« èŠ‚ï¼Œå…± {total_paragraphs} ä¸ªæ®µè½")
        else:
            print("âŒ æ²¡æœ‰æˆåŠŸè·å–ä»»ä½•ç« èŠ‚å†…å®¹")
            print("\nğŸ’¡ å»ºè®®:")
            print("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            print("   2. ç¨åé‡è¯•ï¼ˆå¯èƒ½é‡åˆ°é¢‘ç‡é™åˆ¶ï¼‰")
            print("   3. å°è¯•ä½¿ç”¨VPNæˆ–æ›´æ¢IP")
            print("   4. æ£€æŸ¥ç›®æ ‡ç½‘ç«™æ˜¯å¦æ­£å¸¸è®¿é—®")
    
    def save_to_file(self, chapters_data, filename="æµå…¬å…¨ä¼ .txt"):
        """ä¿å­˜å†…å®¹åˆ°æ–‡ä»¶ - ä¿æŒæ®µè½æ ¼å¼"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write("æµå…¬å…¨ä¼ \n")
                f.write("=" * 60 + "\n")
                f.write("ä½œè€…ï¼šéƒ­å°äº­ï¼ˆæ¸…ä»£ï¼‰\n")
                f.write("çˆ¬å–æ—¶é—´ï¼š" + time.strftime('%Y-%m-%d %H:%M:%S') + "\n")
                f.write("è¯´æ˜ï¼šä¸¥æ ¼æŒ‰ç…§åŸç½‘ç«™pæ ‡ç­¾åˆ†æ®µä¿å­˜\n")
                f.write("=" * 60 + "\n\n")
                
                total_chars = 0
                total_paragraphs = 0
                
                for i, chapter in enumerate(chapters_data, 1):
                    f.write(f"{chapter['title']}\n")
                    f.write("-" * 50 + "\n\n")
                    
                    if chapter['content']:
                        f.write(chapter['content'])  # å†…å®¹å·²ç»æŒ‰æ®µè½æ ¼å¼åŒ–ï¼Œç›´æ¥å†™å…¥
                        total_chars += len(chapter['content'])
                        total_paragraphs += chapter.get('paragraph_count', 0)
                    else:
                        f.write("[æ­¤ç« èŠ‚å†…å®¹è·å–å¤±è´¥]")
                    
                    f.write(f"\n\n\n")
                
                f.write(f"\n\næ€»è®¡: {len(chapters_data)} ç« , {total_chars:,} å­—, {total_paragraphs} æ®µè½\n")
            
            print(f"âœ… TXTæ–‡ä»¶å·²ä¿å­˜: {filename}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜TXTæ–‡ä»¶å¤±è´¥: {e}")
    
    def save_to_json(self, chapters_data, filename="æµå…¬å…¨ä¼ .json"):
        """ä¿å­˜ä¸ºJSONæ ¼å¼"""
        try:
            data = {
                'title': 'æµå…¬å…¨ä¼ ',
                'author': 'éƒ­å°äº­',
                'crawl_time': time.strftime('%Y-%m-%d %H:%M:%S'),
                'format_note': 'ä¸¥æ ¼æŒ‰ç…§åŸç½‘ç«™pæ ‡ç­¾åˆ†æ®µ',
                'total_chapters': len(chapters_data),
                'success_chapters': len([ch for ch in chapters_data if ch.get('success', False)]),
                'total_chars': sum(ch['char_count'] for ch in chapters_data),
                'total_paragraphs': sum(ch.get('paragraph_count', 0) for ch in chapters_data),
                'chapters': chapters_data
            }
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… JSONæ–‡ä»¶å·²ä¿å­˜: {filename}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜JSONå¤±è´¥: {e}")
    
    def save_to_word(self, chapters_data, filename="æµå…¬å…¨ä¼ .docx"):
        """ç›´æ¥ä¿å­˜ä¸ºWordæ–‡æ¡£ - æ ¼å¼åŒ–ç« èŠ‚æ ‡é¢˜ï¼ˆåŠ ç²—ï¼‰"""
        try:
            print("ğŸ“ æ­£åœ¨ç”ŸæˆWordæ–‡æ¡£...")
            
            # åˆ›å»ºæ–°çš„Wordæ–‡æ¡£
            document = Document()
            
            # è®¾ç½®ä¸­æ–‡å­—ä½“
            document.styles['Normal'].font.name = 'å®‹ä½“'
            document.styles['Normal']._element.rPr.rFonts.set(qn('w:eastAsia'), 'å®‹ä½“')
            document.styles['Normal'].font.size = Pt(12)
            
            # æ·»åŠ æ–‡æ¡£æ ‡é¢˜
            title = document.add_heading('æµå…¬å…¨ä¼ ', 0)
            title.alignment = WD_ALIGN_PARAGRAPH.CENTER
            title.runs[0].font.name = 'é»‘ä½“'
            title.runs[0]._element.rPr.rFonts.set(qn('w:eastAsia'), 'é»‘ä½“')
            title.runs[0].font.size = Pt(22)
            title.runs[0].bold = True
            
            # æ·»åŠ ä½œè€…ä¿¡æ¯
            author_para = document.add_paragraph()
            author_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
            author_run = author_para.add_run('ä½œè€…ï¼šéƒ­å°äº­ï¼ˆæ¸…ä»£ï¼‰')
            author_run.font.name = 'æ¥·ä½“'
            author_run._element.rPr.rFonts.set(qn('w:eastAsia'), 'æ¥·ä½“')
            author_run.font.size = Pt(14)
            author_run.italic = True
            
            # æ·»åŠ çˆ¬å–ä¿¡æ¯
            info_para = document.add_paragraph()
            info_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
            info_run = info_para.add_run(f'çˆ¬å–æ—¶é—´ï¼š{time.strftime("%Y-%m-%d %H:%M:%S")}')
            info_run.font.name = 'ä»¿å®‹'
            info_run._element.rPr.rFonts.set(qn('w:eastAsia'), 'ä»¿å®‹')
            info_run.font.size = Pt(10)
            
            # æ·»åŠ åˆ†å‰²çº¿
            document.add_paragraph('â”€' * 50).alignment = WD_ALIGN_PARAGRAPH.CENTER
            document.add_paragraph()  # ç©ºè¡Œ
            
            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            stats_heading = document.add_heading('ä¹¦ç±ç»Ÿè®¡', 1)
            stats_heading.runs[0].bold = True
            stats_heading.runs[0].font.name = 'é»‘ä½“'
            stats_heading.runs[0]._element.rPr.rFonts.set(qn('w:eastAsia'), 'é»‘ä½“')
            
            total_chapters = len(chapters_data)
            success_chapters = len([ch for ch in chapters_data if ch.get('success', False)])
            total_chars = sum(ch.get('char_count', 0) for ch in chapters_data)
            total_paragraphs = sum(ch.get('paragraph_count', 0) for ch in chapters_data)
            
            stats_text = f"""æ€»ç« èŠ‚æ•°ï¼š{total_chapters}
æˆåŠŸç« èŠ‚ï¼š{success_chapters}
æ€»å­—ç¬¦æ•°ï¼š{total_chars:,}
æ€»æ®µè½æ•°ï¼š{total_paragraphs}
æ ¼å¼è¯´æ˜ï¼šä¸¥æ ¼æŒ‰ç…§åŸç½‘ç«™pæ ‡ç­¾åˆ†æ®µ"""
            
            stats_para = document.add_paragraph(stats_text)
            stats_para.runs[0].font.size = Pt(11)
            stats_para.runs[0].font.name = 'ä»¿å®‹'
            stats_para.runs[0]._element.rPr.rFonts.set(qn('w:eastAsia'), 'ä»¿å®‹')
            
            document.add_paragraph()  # ç©ºè¡Œ
            
            # æ·»åŠ ç« èŠ‚å†…å®¹
            success_count = 0
            
            for i, chapter in enumerate(chapters_data, 1):
                if not chapter.get('success', False) or not chapter.get('content'):
                    continue
                    
                success_count += 1
                print(f"ğŸ“„ æ­£åœ¨å¤„ç†ç¬¬ {success_count} ç« : {chapter['title']}")
                
                # ğŸ¯ é‡ç‚¹ï¼šç« èŠ‚æ ‡é¢˜åŠ ç²—
                chapter_heading = document.add_heading(chapter['title'], 1)
                chapter_heading.runs[0].bold = True  # ç¡®ä¿æ ‡é¢˜åŠ ç²—
                chapter_heading.runs[0].font.name = 'é»‘ä½“'
                chapter_heading.runs[0]._element.rPr.rFonts.set(qn('w:eastAsia'), 'é»‘ä½“')
                chapter_heading.runs[0].font.size = Pt(16)
                
                # ç« èŠ‚å†…å®¹ - æŒ‰æ®µè½åˆ†å‰²
                content = chapter['content']
                paragraphs = content.split('\n\n')  # æŒ‰åŒæ¢è¡Œåˆ†æ®µ
                
                for para_text in paragraphs:
                    para_text = para_text.strip()
                    if para_text:
                        # åˆ›å»ºæ®µè½
                        para = document.add_paragraph()
                        para_run = para.add_run(para_text)
                        para_run.font.name = 'å®‹ä½“'
                        para_run._element.rPr.rFonts.set(qn('w:eastAsia'), 'å®‹ä½“')
                        para_run.font.size = Pt(12)
                        
                        # è®¾ç½®æ®µè½æ ¼å¼
                        para.paragraph_format.first_line_indent = Inches(0.5)  # é¦–è¡Œç¼©è¿›
                        para.paragraph_format.space_after = Pt(6)  # æ®µåé—´è·
                        para.paragraph_format.line_spacing = 1.5  # è¡Œè·
                
                # ç« èŠ‚ä¹‹é—´æ·»åŠ åˆ†é¡µç¬¦ï¼ˆé™¤äº†æœ€åä¸€ç« ï¼‰
                if success_count < success_chapters:
                    document.add_page_break()
            
            # ä¿å­˜Wordæ–‡æ¡£
            document.save(filename)
            print(f"âœ… Wordæ–‡æ¡£å·²ä¿å­˜: {filename}")
            print(f"ğŸ“Š æˆåŠŸå¤„ç† {success_count} ä¸ªç« èŠ‚ï¼Œæ ‡é¢˜å·²åŠ ç²—")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜Wordæ–‡æ¡£å¤±è´¥: {e}")
            print("ğŸ’¡ å¯èƒ½æ˜¯python-docxåº“é—®é¢˜ï¼Œè¯·æ£€æŸ¥å®‰è£…ï¼špip install python-docx")

def main():
    """ä¸»å‡½æ•° - åŒ…å«å®Œæ•´çš„ç”¨æˆ·é€‰æ‹©ç•Œé¢"""
    print("=" * 60)
    print("        æµå…¬å…¨ä¼  - ä¸“ä¸šç‰ˆçˆ¬è™« v2.1")
    print("        (è‡ªåŠ¨ç”ŸæˆWordæ–‡æ¡£ç‰ˆ)")
    print("=" * 60)
    print("ğŸ¯ é’ˆå¯¹å¤è¯—æ–‡ç½‘ä¼˜åŒ–ï¼Œä¸¥æ ¼æŒ‰pæ ‡ç­¾åˆ†æ®µ")
    print("ğŸ“ çˆ¬å–å®Œæˆåè‡ªåŠ¨ç”Ÿæˆæ ¼å¼åŒ–Wordæ–‡æ¡£")
    print("âš ï¸  è¯·éµå®ˆç½‘ç«™ä½¿ç”¨æ¡æ¬¾ï¼Œä»…ç”¨äºå­¦ä¹ ç ”ç©¶")
    print("=" * 60)
    
    crawler = JigongCrawler()
    
    # è¯¢é—®ç”¨æˆ·æƒ³è¦çš„æ¨¡å¼
    while True:
        print("\nè¯·é€‰æ‹©çˆ¬å–æ¨¡å¼:")
        print("1. ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼ˆåªçˆ¬å–å‰3ç« ï¼Œå¿«é€ŸéªŒè¯æ•ˆæœï¼‰")
        print("2. ğŸš€ å®Œæ•´æ¨¡å¼ï¼ˆçˆ¬å–æ‰€æœ‰ç« èŠ‚ï¼Œç”Ÿæˆå®Œæ•´Wordæ–‡æ¡£ï¼‰")
        print("3. ğŸ“Š å…ˆæµ‹è¯•å†å†³å®šï¼ˆæ¨èï¼‰")
        print("4. âŒ é€€å‡ºç¨‹åº")
        
        try:
            choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1/2/3/4): ").strip()
            
            if choice == "1":
                print("\n" + "="*50)
                print("ğŸ§ª å·²é€‰æ‹©ï¼šæµ‹è¯•æ¨¡å¼")
                print("ğŸ“ å°†çˆ¬å–å‰3ç« å¹¶ç”Ÿæˆæµ‹è¯•Wordæ–‡æ¡£...")
                crawler.crawl_book(delay=2, test_mode=True)
                break
                
            elif choice == "2":
                print("\n" + "="*50)
                print("ğŸš€ å·²é€‰æ‹©ï¼šå®Œæ•´æ¨¡å¼")
                print("ğŸ“š å°†çˆ¬å–æ‰€æœ‰ç« èŠ‚å¹¶ç”Ÿæˆå®Œæ•´Wordæ–‡æ¡£...")
                
                # äºŒæ¬¡ç¡®è®¤
                confirm = input("âš ï¸  å®Œæ•´çˆ¬å–å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œç¡®è®¤ç»§ç»­ï¼Ÿ(y/n): ").strip().lower()
                if confirm in ['y', 'yes', 'æ˜¯', 'ç¡®è®¤']:
                    crawler.crawl_book(delay=3, test_mode=False)
                    break
                else:
                    print("âŒ å·²å–æ¶ˆå®Œæ•´çˆ¬å–ï¼Œè¿”å›ä¸»èœå•")
                    continue
                
            elif choice == "3":
                print("\n" + "="*50)
                print("ğŸ“Š æ¨èæ¨¡å¼ï¼šå…ˆæµ‹è¯•å†å†³å®š")
                print("ğŸ§ª é¦–å…ˆè¿›è¡Œæµ‹è¯•æ¨¡å¼ï¼ˆå‰3ç« ï¼‰...")
                crawler.crawl_book(delay=2, test_mode=True)
                
                print("\n" + "="*40)
                print("ğŸ“‹ æµ‹è¯•é˜¶æ®µå®Œæˆï¼")
                continue_choice = input("âœ¨ æ•ˆæœæ»¡æ„å—ï¼Ÿæ˜¯å¦ç»§ç»­çˆ¬å–å®Œæ•´ç‰ˆæœ¬ï¼Ÿ(y/n): ").strip().lower()
                
                if continue_choice in ['y', 'yes', 'æ˜¯', 'æ»¡æ„']:
                    print("\nğŸš€ å¼€å§‹å®Œæ•´çˆ¬å–å¹¶ç”Ÿæˆå®Œæ•´Wordæ–‡æ¡£...")
                    crawler.crawl_book(delay=3, test_mode=False)
                else:
                    print("ğŸ‘‹ æµ‹è¯•å®Œæˆï¼Œæ„Ÿè°¢ä½¿ç”¨ï¼")
                break
                
            elif choice == "4":
                print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨æµå…¬å…¨ä¼ çˆ¬è™«ï¼å†è§ï¼")
                break
                
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1ã€2ã€3 æˆ– 4")
                
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­ï¼Œå†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ è¾“å…¥å¤„ç†é”™è¯¯: {e}")
            print("è¯·é‡æ–°é€‰æ‹©...")
            continue
    
    print(f"\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"   ğŸ“„ æµå…¬å…¨ä¼ .txt - çº¯æ–‡æœ¬æ ¼å¼")
    print(f"   ğŸ“‹ æµå…¬å…¨ä¼ .json - æ•°æ®æ ¼å¼ï¼ˆåŒ…å«å…ƒæ•°æ®ï¼‰")
    if DOCX_AVAILABLE:
        print(f"   ğŸ“ æµå…¬å…¨ä¼ .docx - Wordæ–‡æ¡£ï¼ˆæ ‡é¢˜åŠ ç²—ï¼Œæ ¼å¼åŒ–ï¼‰")
    else:
        print(f"   âš ï¸  Wordæ–‡æ¡£æœªç”Ÿæˆï¼ˆéœ€è¦å®‰è£…ï¼špip install python-docxï¼‰")
    
    print(f"\nğŸ‰ æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åœ¨å½“å‰ç›®å½•ï¼")

if __name__ == "__main__":
    main()